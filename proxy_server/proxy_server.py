"""
proxy_server.py - LLM í†µì‹  ì¤‘ê³„ ë° ê·œê²© ë³€í™˜ ë©”ì¸ ì„œë²„

Void IDEì™€ Ollama ì‚¬ì´ì—ì„œ ë©”ì‹œì§€ë¥¼ ì¤‘ê³„í•˜ê³ ,
ìš”ì²­/ì‘ë‹µ í˜•ì‹ì„ ë³€í™˜í•˜ë©°, ëª¨ë“  í†µì‹ ì„ ë¡œê¹…í•©ë‹ˆë‹¤.

ì‹¤í–‰ ë°©ë²•:
    uvicorn proxy_server:app --host 127.0.0.1 --port 8000 --reload
"""

import json
import logging
import sys
from datetime import datetime
from pathlib import Path
 # í˜„ì¬ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent))
from typing import Dict, Any, List, Optional

import httpx
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
from pydantic import BaseModel

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
from proxy_adapter import OllamaAdapter, RequestValidator
from inventory import get_inventory, ToolInventory

# ì„¤ì • íŒŒì¼ ë¡œë“œ
CONFIG_PATH = Path(__file__).parent / "proxy_config" / "proxy_config.json"

def load_config() -> Dict[str, Any]:
    """ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        with open(CONFIG_PATH) as f:
            config = json.load(f)
            
        # í”„ë¡œíŒŒì¼ ì§€ì›: active_profileì´ ìˆìœ¼ë©´ í•´ë‹¹ ì„¤ì •ì„ llm ì„¹ì…˜ìœ¼ë¡œ ë³µì‚¬
        if "active_profile" in config and "llm_profiles" in config:
            active = config["active_profile"]
            if active in config["llm_profiles"]:
                config["llm"] = config["llm_profiles"][active]
                
        return config
    except Exception as e:
        print(f"[Config] ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
        return {
            "llm": {
                "provider": "ollama",
                "base_url": "http://127.0.0.1:11434/v1",
                "model": "qwen2.5-coder:7b",
                "api_key": "not-needed",
                "timeout": 60
            },
            "proxy": {
                "host": "127.0.0.1",
                "port": 8000
            },
            "logging": {
                "level": "DEBUG"
            }
        }

config = load_config()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=getattr(logging, config["logging"]["level"]),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler("proxy_server.log", encoding="utf-8")
    ]
)
logger = logging.getLogger("proxy_server")


@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™” ë° ì¢…ë£Œ ì‹œ ì •ë¦¬"""
    logger.info("=" * 60)
    logger.info("ğŸš€ Proxy Server ì‹œì‘")
    logger.info(f"LLM Provider: {config['llm']['provider']}")
    logger.info(f"LLM Base URL: {config['llm']['base_url']}")
    logger.info(f"Default Model: {config['llm']['model']}")
    logger.info(f"Database Path: {config.get('database', {}).get('path', 'Not Configured')}")
    logger.info("=" * 60)
    
    # MCP ì„œë²„ì—ì„œ ë„êµ¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    inventory = get_inventory()
    try:
        tools = await inventory.fetch_tools_from_mcp()
        logger.info(f"ğŸ“¦ ì¸ë²¤í† ë¦¬ ë¡œë“œ ì™„ë£Œ: {len(tools)}ê°œ ë„êµ¬")
    except Exception as e:
        logger.warning(f"âš ï¸ MCP ì„œë²„ ì—°ê²° ì‹¤íŒ¨, ê¸°ë³¸ ë„êµ¬ ì‚¬ìš©: {e}")
    
    yield
    logger.info("ğŸ‘‹ Proxy Server ì¢…ë£Œ")


# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="Void Lab Test - Proxy Server",
    description="LLM í†µì‹  ì¤‘ê³„ ë° ê·œê²© ë³€í™˜ ì„œë²„",
    version="1.0.0",
    lifespan=lifespan
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
class ChatMessage(BaseModel):
    role: str
    content: Optional[str] = None
    tool_calls: Optional[List[Dict[str, Any]]] = None
    tool_call_id: Optional[str] = None
    name: Optional[str] = None


class ChatRequest(BaseModel):
    model: Optional[str] = None
    messages: List[ChatMessage]
    tools: Optional[List[Dict[str, Any]]] = None
    stream: bool = False


class ChatResponse(BaseModel):
    id: str
    object: str
    created: Optional[str] = None
    model: str
    choices: List[Dict[str, Any]]
    usage: Dict[str, int]


# ì–´ëŒ‘í„° ë° ì¸ë²¤í† ë¦¬ ì¸ìŠ¤í„´ìŠ¤
adapter = OllamaAdapter()
validator = RequestValidator()


@app.get("/")
async def root():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "status": "running",
        "service": "Void Lab Test Proxy Server",
        "timestamp": datetime.now().isoformat()
    }


@app.get("/v1/models")
async def list_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
    return {
        "object": "list",
        "data": [
            {
                "id": config["llm"]["model"],
                "object": "model",
                "created": int(datetime.now().timestamp()),
                "owned_by": config["llm"]["provider"]
            }
        ]
    }


@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest, raw_request: Request):
    """
    ì±„íŒ… ì™„ì„± ì—”ë“œí¬ì¸íŠ¸ (OpenAI í˜¸í™˜)
    
    ğŸ” ë¶„ì„ í¬ì¸íŠ¸ 1: Voidê°€ ë³´ë‚¸ íˆ´ ëª…ì„¸ì™€ ì§ˆë¬¸ ë‚´ìš© í™•ì¸
    ğŸ” ë¶„ì„ í¬ì¸íŠ¸ 2: LLM ì‘ë‹µì—ì„œ ë„êµ¬ í˜¸ì¶œ ì—¬ë¶€ í™•ì¸
    """
    request_id = datetime.now().strftime("%H%M%S%f")
    
    # ìš”ì²­ ë¡œê¹…
    logger.info("=" * 60)
    logger.info(f"ğŸ“¥ [REQ-{request_id}] ìƒˆ ìš”ì²­ ìˆ˜ì‹ ")
    logger.info(f"   ëª¨ë¸: {request.model or config['llm']['model']}")
    logger.info(f"   ë©”ì‹œì§€ ìˆ˜: {len(request.messages)}")
    logger.info(f"   ë„êµ¬ ìˆ˜: {len(request.tools) if request.tools else 0}")
    
    # ë©”ì‹œì§€ ë‚´ìš© ìƒì„¸ ë¡œê¹…
    for i, msg in enumerate(request.messages):
        role = msg.role
        content = msg.content[:100] if msg.content else "(no content)"
        logger.debug(f"   ë©”ì‹œì§€[{i}] {role}: {content}...")
    
    # ìš”ì²­ ìœ íš¨ì„± ê²€ì¦
    is_valid, error = validator.validate_chat_request(request.dict())
    if not is_valid:
        logger.error(f"âŒ ìš”ì²­ ìœ íš¨ì„± ê²€ì¦ ì‹¤íŒ¨: {error}")
        raise HTTPException(status_code=400, detail=error)
    
    # ë„êµ¬ ëª©ë¡ ì¤€ë¹„ (ìš”ì²­ì— ì—†ìœ¼ë©´ ì¸ë²¤í† ë¦¬ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
    tools = request.tools
    if not tools:
        inventory = get_inventory()
        tools = inventory.get_tools_for_llm()
        logger.info(f"ğŸ“¦ ì¸ë²¤í† ë¦¬ì—ì„œ ë„êµ¬ ë¡œë“œ: {len(tools)}ê°œ")
    
    # LLM ìš”ì²­ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì–´ëŒ‘í„°ëŠ” OpenAI ê·œê²©ì„ ë”°ë¦„)
    ollama_request = adapter.convert_to_ollama_request(
        messages=[msg.dict(exclude_none=True) for msg in request.messages],
        tools=tools,
        model=request.model or config["llm"]["model"],
        stream=request.stream
    )
    
    logger.info(f"ğŸ”„ [REQ-{request_id}] LLMìœ¼ë¡œ ìš”ì²­ ì „ì†¡ ì¤‘...")
    logger.debug(f"   URL: {config['llm']['base_url']}/chat/completions")
    logger.debug(f"   ìš”ì²­: {json.dumps(ollama_request, ensure_ascii=False, indent=2)}")
    
    # Ollama API í˜¸ì¶œ
    try:
        if request.stream:
            async def stream_generator():
                async with httpx.AsyncClient(timeout=config["llm"]["timeout"]) as client:
                    llm_url = f"{config['llm']['base_url']}/chat/completions"
                    headers = {}
                    if config["llm"].get("api_key") and config["llm"]["api_key"] != "not-needed":
                        headers["Authorization"] = f"Bearer {config['llm']['api_key']}"
                        
                    async with client.stream("POST", llm_url, json=ollama_request, headers=headers) as response:
                        response.raise_for_status()
                        async for line in response.aiter_lines():
                            if not line:
                                continue
                            if line.startswith("data: "):
                                data = line[6:]
                                if data == "[DONE]":
                                    break
                                try:
                                    chunk = json.loads(data)
                                    converted_chunk = adapter.convert_chunk_from_ollama(chunk)
                                    logger.debug(f"ğŸ“¡ [REQ-{request_id}] ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ ë³€í™˜ ì™„ë£Œ")
                                    yield converted_chunk
                                except json.JSONDecodeError:
                                    logger.error(f"âŒ [REQ-{request_id}] ì²­í¬ íŒŒì‹± ì‹¤íŒ¨: {data}")
                                    continue
                            else:
                                logger.info(f"â„¹ï¸ [REQ-{request_id}] ë¹„-ë°ì´í„° ë¼ì¸(Full JSON) ìˆ˜ì‹ ")
                                try:
                                    # Ollamaê°€ stream: falseë¡œ ì‘ë‹µí•˜ì—¬ JSON í•œ ì¤„ì´ ì™”ì„ ê²½ìš° ì²˜ë¦¬
                                    full_resp_raw = json.loads(line)
                                    # 1. Ollama -> OpenAI Full Response ë³€í™˜ (ë„êµ¬ ì¶”ì¶œ í¬í•¨)
                                    openai_full = adapter.convert_from_ollama_response(full_resp_raw)
                                    # 2. OpenAI Full Response -> OpenAI Chunks ë³€í™˜ (ë¦¬ìŠ¤íŠ¸ ë°˜í™˜)
                                    converted_chunks = adapter.convert_to_chunk_from_full_response(openai_full)
                                    logger.info(f"ğŸ“¡ [REQ-{request_id}] ë¹„-ë°ì´í„° ì‘ë‹µì„ {len(converted_chunks)}ê°œì˜ ì²­í¬ë¡œ ë¡œ ë¶„í• í•˜ì—¬ ì „ì†¡í•©ë‹ˆë‹¤.")
                                    for idx, chunk in enumerate(converted_chunks):
                                        logger.debug(f"   ì²­í¬[{idx}]: {chunk}")
                                        yield chunk
                                except Exception as e:
                                    logger.error(f"âŒ [REQ-{request_id}] ë¹„-ë°ì´í„° ë¼ì¸ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
                                    continue
                
                logger.debug(f"ğŸ [REQ-{request_id}] ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì‹ í˜¸ ì „ì†¡")
                yield "data: [DONE]\n\n"

            logger.info(f"ğŸ“¡ [REQ-{request_id}] ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‹œì‘")
            return StreamingResponse(stream_generator(), media_type="text/event-stream")

        else:
            async with httpx.AsyncClient(timeout=config["llm"]["timeout"]) as client:
                llm_url = f"{config['llm']['base_url']}/chat/completions"
                headers = {}
                if config["llm"].get("api_key") and config["llm"]["api_key"] != "not-needed":
                    headers["Authorization"] = f"Bearer {config['llm']['api_key']}"
                    
                response = await client.post(llm_url, json=ollama_request, headers=headers)
                response.raise_for_status()
                
                ollama_response = response.json()
            
            # ì‘ë‹µ ë³€í™˜
            openai_response = adapter.convert_from_ollama_response(ollama_response)
            
            # ì‘ë‹µ ë¡œê¹…
            logger.info(f"ğŸ“¤ [REQ-{request_id}] ì‘ë‹µ ë°˜í™˜")
            
            # ë„êµ¬ í˜¸ì¶œ ì—¬ë¶€ í™•ì¸ (ë¶„ì„ í¬ì¸íŠ¸ 2)
            tool_calls = adapter.extract_tool_calls(openai_response)
            if tool_calls:
                logger.info(f"ğŸ”§ [REQ-{request_id}] LLMì´ ë„êµ¬ í˜¸ì¶œ ìš”ì²­!")
                for tc in tool_calls:
                    func = tc.get("function", {})
                    logger.info(f"   â†’ {func.get('name')}: {func.get('arguments')}")
            else:
                content = openai_response["choices"][0]["message"].get("content", "")
                logger.info(f"ğŸ’¬ [REQ-{request_id}] ì¼ë°˜ ì‘ë‹µ: {content[:100]}...")
            
            logger.info("=" * 60)
            return openai_response

    except httpx.ReadTimeout:
        logger.error(f"â±ï¸ [REQ-{request_id}] LLM ì‘ë‹µ ì‹œê°„ ì´ˆê³¼ (ReadTimeout)")
        if request.stream:
            async def error_generator():
                yield f"data: {json.dumps({'error': 'LLM ì‘ë‹µ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë¸ ë¡œë”© ì¤‘ì´ê±°ë‚˜ ì„œë²„ ë¶€í•˜ê°€ ë†’ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.'}, ensure_ascii=False)}\n\n"
                yield "data: [DONE]\n\n"
            return StreamingResponse(error_generator(), media_type="text/event-stream")
        else:
            raise HTTPException(status_code=504, detail="LLM ì‘ë‹µ ì‹œê°„ ì´ˆê³¼ (ReadTimeout)")
            
    except httpx.ConnectTimeout:
        logger.error(f"â±ï¸ [REQ-{request_id}] LLM ì—°ê²° ì‹œê°„ ì´ˆê³¼ (ConnectTimeout)")
        raise HTTPException(status_code=504, detail="LLM ì—°ê²° ì‹œê°„ ì´ˆê³¼")
        
    except httpx.TimeoutException:
        logger.error(f"â±ï¸ [REQ-{request_id}] LLM ê¸°íƒ€ íƒ€ì„ì•„ì›ƒ ë°œìƒ")
        raise HTTPException(status_code=504, detail="LLM ìš”ì²­ ì‹œê°„ ì´ˆê³¼")
        
    except httpx.HTTPError as e:
        logger.error(f"âŒ [REQ-{request_id}] LLM ì—°ê²° ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=502, detail=f"LLM ì—°ê²° ì‹¤íŒ¨: {str(e)}")


@app.get("/tools")
async def get_tools():
    """ë“±ë¡ëœ ë„êµ¬ ëª©ë¡ ì¡°íšŒ"""
    inventory = get_inventory()
    return {
        "tools": inventory.get_tools_for_llm()
    }


@app.post("/tools/refresh")
async def refresh_tools():
    """MCP ì„œë²„ì—ì„œ ë„êµ¬ ëª©ë¡ ìƒˆë¡œê³ ì¹¨"""
    inventory = get_inventory()
    try:
        tools = await inventory.fetch_tools_from_mcp()
        return {
            "status": "success",
            "tools_count": len(tools)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "proxy_server:app",
        host=config["proxy"]["host"],
        port=config["proxy"]["port"],
        reload=True,
        log_level="debug"
    )
