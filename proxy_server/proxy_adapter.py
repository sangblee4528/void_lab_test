"""
proxy_adapter.py - ëª¨ë¸ë³„ í˜¸ì¶œ ê·œê²© ë³€í™˜ ë¡œì§

Voidì—ì„œ ë°›ì€ ìš”ì²­ì„ Ollama API ê·œê²©ìœ¼ë¡œ ë³€í™˜í•˜ê³ ,
Ollama ì‘ë‹µì„ Voidê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
"""

import json
import logging
import re
from datetime import datetime
from pathlib import Path
import sys

# í˜„ì¬ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent))
from typing import Dict, Any, List, Optional

logger = logging.getLogger(__name__)

# í”„ë¡¬í”„íŠ¸ ì„¤ì • ê²½ë¡œ
PROMPT_CONFIG_PATH = Path(__file__).parent / "proxy_config" / "prompt_config.json"


class OllamaAdapter:
    """Ollama API ê·œê²© ë³€í™˜ ì–´ëŒ‘í„°"""
    
    @staticmethod
    def convert_to_ollama_request(
        messages: List[Dict[str, Any]],
        tools: Optional[List[Dict[str, Any]]] = None,
        model: str = "qwen2.5-coder:7b",
        stream: bool = False
    ) -> Dict[str, Any]:
        """
        OpenAI í˜•ì‹ì˜ ìš”ì²­ì„ Ollama API í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        Args:
            messages: ëŒ€í™” ë©”ì‹œì§€ ëª©ë¡ (OpenAI í˜•ì‹)
            tools: ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡
            model: ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„
            stream: ìŠ¤íŠ¸ë¦¬ë° ì—¬ë¶€
            
        Returns:
            Dict: Ollama API ìš”ì²­ í˜•ì‹
        """
        logger.info(f"[Adapter] OpenAI â†’ Ollama(OpenAI) ë³€í™˜ ì‹œì‘")
        logger.debug(f"[Adapter] ì›ë³¸ ë©”ì‹œì§€: {json.dumps(messages, ensure_ascii=False, indent=2)}")
        
        # [ìƒì„¸ ì½”ë©˜íŠ¸: ì„¤ì • íŒŒì¼ì„ ì´ìš©í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì£¼ì…]
        # ì™¸ë¶€ prompt_config.json íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ë„êµ¬ ì‚¬ìš© ê¶Œì¥ íŒíŠ¸ë¥¼ ì£¼ì…í•©ë‹ˆë‹¤.
        injected_messages = messages
        if tools and PROMPT_CONFIG_PATH.exists():
            try:
                with open(PROMPT_CONFIG_PATH, "r", encoding="utf-8") as f:
                    p_config = json.load(f)
                
                hint_cfg = p_config.get("system_hint", {})
                if hint_cfg.get("enabled", False):
                    tool_names = ", ".join([t.get('function', {}).get('name', 'tool') for t in tools])
                    raw_hint = hint_cfg.get("content", "")
                    
                    # ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì´ë©´ ê°œí–‰ë¬¸ìë¡œ í•©ì¹¨, ë¬¸ìì—´ì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    if isinstance(raw_hint, list):
                        raw_hint = "\n".join(raw_hint)
                        
                    tool_hint = raw_hint.replace("{tool_names}", tool_names)
                    
                    # ìƒˆë¡œìš´ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ ìƒì„± (ê¸°ì¡´ ë©”ì‹œì§€ ë³€ê²½ ë°©ì§€)
                    new_messages = []
                    system_msg_found = False
                    for msg in messages:
                        m = msg.copy()
                        if m.get("role") == "system" and not system_msg_found:
                            m["content"] = (m.get("content") or "") + tool_hint
                            system_msg_found = True
                        new_messages.append(m)
                    
                    if not system_msg_found:
                        new_messages.insert(0, {"role": "system", "content": tool_hint})
                    
                    injected_messages = new_messages
                    logger.info("[Adapter] ì™¸ë¶€ ì„¤ì •ì„ í†µí•´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ íŒíŠ¸ ì£¼ì… ì™„ë£Œ")
            except Exception as e:
                logger.error(f"[Adapter] í”„ë¡¬í”„íŠ¸ ì„¤ì • ë¡œë“œ ì¤‘ ì—ëŸ¬: {e}")

        # OpenAI í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ êµ¬ì„±
        ollama_request = {
            "model": model,
            "messages": injected_messages,
            "stream": stream,
            "temperature": 0.7,
            "max_tokens": 1024
        }
        
        # ë„êµ¬ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if tools:
            ollama_request["tools"] = tools
            # [ìƒì„¸ ì½”ë©˜íŠ¸: ë„êµ¬ ì¶”ì¶œì„ ìœ„í•œ ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™”]
            # Void IDEëŠ” ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì—ë„ ë„êµ¬ í˜¸ì¶œì´ í…ìŠ¤íŠ¸ë¡œ ì˜¤ë©´ ê°ì§€í•˜ì§€ ëª»í•©ë‹ˆë‹¤.
            # 8000ë²ˆ í”„ë¡ì‹œì—ì„œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ë„êµ¬ ì¶”ì¶œ(Fallback)ì„ ìˆ˜í–‰í•˜ë ¤ë©´ ì‘ë‹µì„ í•œ ë²ˆì— ë‹¤ ë°›ì•„ì•¼ í•˜ë¯€ë¡œ
            # ë„êµ¬ ëª©ë¡ì´ í¬í•¨ëœ ìš”ì²­ì—ì„œëŠ” ê°•ì œë¡œ streamì„ Falseë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
            ollama_request["stream"] = False
            logger.info(f"[Adapter] ë„êµ¬ {len(tools)}ê°œ í¬í•¨ë¨ (í…ìŠ¤íŠ¸ ì¶”ì¶œì„ ìœ„í•´ ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™”)")
        
        logger.info(f"[Adapter] Ollama ìš”ì²­ ë³€í™˜ ì™„ë£Œ")
        return ollama_request
    
    @staticmethod
    def convert_from_ollama_response(
        ollama_response: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Ollama API ì‘ë‹µì„ OpenAI í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        Args:
            ollama_response: Ollama API ì‘ë‹µ
            
        Returns:
            Dict: OpenAI í˜•ì‹ì˜ ì‘ë‹µ
        """
        logger.info(f"[Adapter] Ollama â†’ OpenAI ë³€í™˜ ì‹œì‘")
        # [ìƒì„¸ ì½”ë©˜íŠ¸: ì›ë³¸ ì‘ë‹µ í™•ì¸]
        # ëª¨ë¸ì´ ì‹¤ì œë¡œ ë³´ë‚¸ 'ë‚ ê²ƒ'ì˜ ì‘ë‹µì„ í™•ì¸í•˜ê¸° ìœ„í•´ ë¡œê·¸ ë ˆë²¨ì„ INFOë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
        logger.info(f"[Adapter] [RAW] Ollama ì›ë³¸ ì‘ë‹µ: {json.dumps(ollama_response, ensure_ascii=False, indent=2)}")
        
        # [ìƒì„¸ ì½”ë©˜íŠ¸: ì‘ë‹µ í¬ë§· ëŒ€ì‘]
        # Ollama ë„¤ì´í‹°ë¸Œ APIëŠ” 'message'ë¥¼ ì§ì ‘ ë°˜í™˜í•˜ì§€ë§Œ, 
        # OpenAI í˜¸í™˜ API(/v1/)ëŠ” 'choices[0].message' êµ¬ì¡°ë¥¼ ê°–ìŠµë‹ˆë‹¤.
        # ë‘ ê²½ìš°ë¥¼ ëª¨ë‘ ì§€ì›í•˜ë„ë¡ ë¡œì§ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
        message = ollama_response.get("message")
        if not message:
            choices = ollama_response.get("choices", [])
            if choices:
                message = choices[0].get("message", {})
            else:
                message = {}
        
        content = message.get("content", "")
        tool_calls = message.get("tool_calls", [])
        
        # [ìƒì„¸ ì½”ë©˜íŠ¸: ë„êµ¬ í˜¸ì¶œ ì¶”ì¶œ í´ë°± ë¡œì§]
        # ëª¨ë¸(ì˜ˆ: Qwen 2.5)ì´ ì •ì‹ tool_calls í•„ë“œ ëŒ€ì‹  ì¼ë°˜ í…ìŠ¤íŠ¸(content) ì•ˆì— JSONìœ¼ë¡œ ë„êµ¬ ì •ë³´ë¥¼ ë³´ë‚¼ ë•Œê°€ ìˆìŠµë‹ˆë‹¤.
        # ì´ ê²½ìš° Void IDEëŠ” ì´ë¥¼ ë„êµ¬ë¡œ ì¸ì‹í•˜ì§€ ëª»í•˜ë¯€ë¡œ, í”„ë¡ì‹œ ë ˆë²¨ì—ì„œ contentë¥¼ ë’¤ì ¸ì„œ JSONì„ ì°¾ì•„ëƒ…ë‹ˆë‹¤.
        if not tool_calls and content:
            # _try_extract_json_tool_call()ë¥¼ í˜¸ì¶œí•˜ì—¬ JSON íŒ¨í„´(ì§ì ‘ JSON í˜¹ì€ ```json ë¸”ë¡)ì„ ì°¾ìŠµë‹ˆë‹¤.
            json_match = re.search(r"```json\s*([\s\S]*?)\s*```", content)
            if json_match:
                extracted = OllamaAdapter._try_extract_json_tool_call(json_match.group(0))
                if extracted:
                    tool_calls = extracted
                    # contentì—ì„œ ì¶”ì¶œëœ JSON ë¸”ë¡ì„ ì œê±°í•˜ì—¬ Voidê°€ 'Apply' ë²„íŠ¼ì„ ë³´ì—¬ì£¼ì§€ ì•Šê²Œ í•¨
                    # ì •ê·œì‹ìœ¼ë¡œ ë” í™•ì‹¤í•˜ê²Œ ì œê±°
                    content = re.sub(r"```json\s*[\s\S]*?\s*```", "", content).strip()
                    logger.info(f"[Adapter] ğŸ’¡ í…ìŠ¤íŠ¸ ì½”ë“œ ë¸”ë¡ì—ì„œ ë„êµ¬ í˜¸ì¶œ ì¶”ì¶œ ë° ë³¸ë¬¸ ì •ì œ ì™„ë£Œ")
            elif content.strip().startswith("{") and content.strip().endswith("}"):
                extracted = OllamaAdapter._try_extract_json_tool_call(content)
                if extracted:
                    tool_calls = extracted
                    content = "" # ì „ì²´ê°€ JSONì´ë©´ ë³¸ë¬¸ ë¹„ì›€
                    logger.info(f"[Adapter] ğŸ’¡ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë„êµ¬ í˜¸ì¶œ ì¶”ì¶œ ì™„ë£Œ")

        openai_response = {
            "id": ollama_response.get("id") or f"chatcmpl-{ollama_response.get('created_at', 'unknown')}",
            "object": "chat.completion",
            "created": ollama_response.get("created") or ollama_response.get("created_at"),
            "model": ollama_response.get("model"),
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": message.get("role", "assistant"),
                        # ë„êµ¬ í˜¸ì¶œì´ ìˆì„ ë•Œ contentê°€ ""ì´ë©´ Voidê°€ 'Empty response'ë¡œ ì˜¤í•´í•  ìˆ˜ ìˆìŒ
                        # ë„êµ¬ í˜¸ì¶œì´ ìˆë”ë¼ë„ ì›ë³¸ contentë¥¼ ìœ ì§€í•˜ê±°ë‚˜, ìµœì†Œí•œ ë¹ˆ ë¬¸ìì—´("")ë¡œ ì²˜ë¦¬
                        # ë‚´ìš©ì´ ì•„ì˜ˆ ì—†ìœ¼ë©´ Voidê°€ ë©”ì‹œì§€ ìì²´ë¥¼ ì”¹ì–´ë²„ë¦¬ëŠ”(ìˆ¨ê¸°ëŠ”) í˜„ìƒ ë°©ì§€
                        "content": content if not tool_calls else (content or "ğŸ› ï¸ ë„êµ¬ í˜¸ì¶œì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
                    },
                    "finish_reason": ollama_response.get("finish_reason") or "stop"
                }
            ],
            "usage": ollama_response.get("usage") or {
                "prompt_tokens": ollama_response.get("prompt_eval_count", 0),
                "completion_tokens": ollama_response.get("eval_count", 0),
                "total_tokens": (
                    ollama_response.get("prompt_eval_count", 0) + 
                    ollama_response.get("eval_count", 0)
                )
            }
        }
        
        # tool_callsê°€ ìˆìœ¼ë©´ ì¶”ê°€ ë° finish_reason ë³€ê²½
        # [Plan B] ë„êµ¬ í˜¸ì¶œì„ ì‰˜ ëª…ë ¹ì–´ë¡œ ë³€í™˜í•˜ì—¬ Contentì— ì£¼ì…
        # Voidê°€ ë„êµ¬ í˜¸ì¶œ JSONì€ ë¬´ì‹œí•˜ì§€ë§Œ, ì‰˜ ëª…ë ¹ì–´ ì½”ë“œëŠ” ì¸ì‹í•˜ì—¬ Run ë²„íŠ¼ì„ ë„ìš°ëŠ” ì ì„ ì´ìš©
        if tool_calls:
            logger.info(f"[Adapter] ğŸ”§ ë„êµ¬ í˜¸ì¶œì„ ì‰˜ ëª…ë ¹ì–´ë¡œ ë³€í™˜ (Plan B): {len(tool_calls)}ê±´")
            
            command_lines = []
            for tool in tool_calls:
                fn_name = tool["function"]["name"]
                fn_args = tool["function"]["arguments"]
                
                # ì¸ì ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
                escaped_args = fn_args.replace("'", "'\\''")
                cmd = f"python mcp_server/mcp_tools_runner.py {fn_name} '{escaped_args}'"
                command_lines.append(f"```bash\n{cmd}\n```")
            
            # ë³¸ë¬¸ì— ì‰˜ ëª…ë ¹ì–´ ì¶”ê°€
            # ì´ë¯¸ placeholder í…ìŠ¤íŠ¸ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì²´í¬
            base_content = content if content else "ğŸ› ï¸ ë„êµ¬ ì‹¤í–‰ì„ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤:"
            openai_response["choices"][0]["message"]["content"] = base_content + "\n\n" + "\n".join(command_lines)
            
            # ì¤‘ìš”: ë„êµ¬ í˜¸ì¶œ í•„ë“œëŠ” ë¹„ì›ë‹ˆë‹¤. Voidê°€ ë„êµ¬ í˜¸ì¶œ UI ëŒ€ì‹  ì‰˜ UIë¥¼ ì“°ë„ë¡ ìœ ë„
            # openai_response["choices"][0]["message"]["tool_calls"] = tool_calls
            # openai_response["choices"][0]["finish_reason"] = "tool_calls"
            
            # finish_reasonì€ stopìœ¼ë¡œ ìœ ì§€
            openai_response["choices"][0]["finish_reason"] = "stop"
        
        logger.info(f"[Adapter] OpenAI ì‘ë‹µ ë³€í™˜ ì™„ë£Œ")
        return openai_response

    @staticmethod
    def _try_extract_json_tool_call(content: str) -> Optional[List[Dict[str, Any]]]:
        """
        [ìƒì„¸ ì½”ë©˜íŠ¸: í…ìŠ¤íŠ¸ ë‚´ JSON ì¶”ì¶œê¸°]
        ëª¨ë¸ì´ ë‹µë³€ ë©”ì‹œì§€ ì•ˆì— ì„ì–´ ë³´ë‚¸ ë„êµ¬ í˜¸ì¶œ ì •ë³´ë¥¼ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì°¾ì•„ëƒ…ë‹ˆë‹¤.
        
        1ìˆœìœ„: ```json ... ``` í˜•íƒœì˜ ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡
        2ìˆœìœ„: ë©”ì‹œì§€ ì „ì²´ê°€ { ... } ë˜ëŠ” [ ... ] ì¸ ê²½ìš°
        """
        content = content.strip()
        
        # 1. ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ë‚´ë¶€ì˜ JSONì„ ë¨¼ì € ì°¾ìŠµë‹ˆë‹¤. (ê°€ì¥ í”í•œ ì¼€ì´ìŠ¤)
        # re.DOTALLì„ ì§€ì›í•˜ê¸° ìœ„í•´ [\s\S]*? íŒ¨í„´ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        json_match = re.search(r"```json\s*([\s\S]*?)\s*```", content)
        if json_match:
            json_str = json_match.group(1).strip()
        else:
            # 2. ì½”ë“œ ë¸”ë¡ì´ ì—†ë‹¤ë©´ ì „ì²´ í…ìŠ¤íŠ¸ê°€ { ë¡œ ì‹œì‘í•´ì„œ } ë¡œ ëë‚˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
            if content.startswith("{") and content.endswith("}"):
                json_str = content
            else:
                return None
        
        try:
            data = json.loads(json_str)
            
            # ì¼€ì´ìŠ¤ A: ë‹¨ì¼ ë„êµ¬ í˜¸ì¶œ ê°ì²´ì¸ ê²½ìš° (Qwen ìŠ¤íƒ€ì¼)
            # { "name": "...", "arguments": { ... } }
            if isinstance(data, dict):
                if "name" in data and "arguments" in data:
                    return [{
                        "index": 0,
                        "id": f"call_{datetime.now().strftime('%M%S%f')}", # Void ì¸ì‹ìš© ID ìƒì„±
                        "type": "function", # OpenAI ê·œê²© ê³ ì •
                        "function": {
                            "name": data["name"],
                            # argumentsëŠ” ë°˜ë“œì‹œ JSON ë¬¸ìì—´ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤.
                            "arguments": json.dumps(data["arguments"], ensure_ascii=False) if isinstance(data["arguments"], dict) else data["arguments"]
                        }
                    }]
            
            # ì¼€ì´ìŠ¤ B: ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ë„êµ¬ í˜¸ì¶œì¸ ê²½ìš° (OpenAI ìŠ¤íƒ€ì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³´ë‚¸ ê²½ìš°)
            # [{ "name": "...", "arguments": { ... } }, ...]
            elif isinstance(data, list):
                valid_calls = []
                for idx, item in enumerate(data):
                    if isinstance(item, dict) and "name" in item:
                        valid_calls.append({
                            "index": idx,  # OpenAI ìŠ¤íŠ¸ë¦¬ë° ê·œê²©ì—ì„œëŠ” indexê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.
                            "id": f"call_{idx}_{datetime.now().strftime('%M%S%f')}",
                            "type": "function",
                            "function": {
                                "name": item["name"],
                                "arguments": json.dumps(item.get("arguments", {}), ensure_ascii=False)
                            }
                        })
                return valid_calls if valid_calls else None
        except Exception as e:
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¬´ì‹œí•˜ê³  ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬í•˜ê²Œ ë‘¡ë‹ˆë‹¤.
            logger.debug(f"[Adapter] JSON ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            
        return None

    @staticmethod
    def convert_chunk_from_ollama(
        ollama_chunk: Dict[str, Any]
    ) -> str:
        """
        Ollama ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ë¥¼ OpenAI SSE í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        Args:
            ollama_chunk: Ollama APIì˜ í•œ ì²­í¬
            
        Returns:
            str: "data: {...}\n\n" í˜•ì‹ì˜ ë¬¸ìì—´
        """
        choices = ollama_chunk.get("choices", [])
        if not choices:
            return ""
            
        choice = choices[0]
        delta = choice.get("delta", {})
        
        openai_chunk = {
            "id": ollama_chunk.get("id", f"chatcmpl-{datetime.now().strftime('%Y%M%S%f')}"),
            "object": "chat.completion.chunk",
            "created": ollama_chunk.get("created"),
            "model": ollama_chunk.get("model"),
            "choices": [
                {
                    "index": 0,
                    "delta": delta,
                    "finish_reason": choice.get("finish_reason")
                }
            ]
        }
        
        return f"data: {json.dumps(openai_chunk, ensure_ascii=False)}\n\n"

    @staticmethod
    def convert_to_chunk_from_full_response(
        openai_response: Dict[str, Any]
    ) -> List[str]:
        """
        [ìƒì„¸ ì½”ë©˜íŠ¸: í’€ ì‘ë‹µ â†’ ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ ë³€í™˜]
        ë„êµ¬ ì¶”ì¶œì„ ìœ„í•´ ê°•ì œë¡œ ë¹„ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¥¼ ì‚¬ìš©í–ˆì„ ë•Œ,
        ì›ë˜ ìŠ¤íŠ¸ë¦¬ë°ì„ ê¸°ëŒ€í•˜ë˜ í´ë¼ì´ì–¸íŠ¸(Void)ë¥¼ ìœ„í•´ ê²°ê³¼ë¥¼ ì—¬ëŸ¬ SSE ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ í¬ì¥í•©ë‹ˆë‹¤.
        ë³¸ë¬¸(content)ê³¼ ë„êµ¬ í˜¸ì¶œ(tool_calls)ì„ ë¶„ë¦¬í•˜ì—¬ ì „ë‹¬í•˜ì—¬ Voidì˜ ì¸ì‹ë¥ ì„ ë†’ì…ë‹ˆë‹¤.
        """
        choices = openai_response.get("choices", [])
        if not choices:
            return []
            return []
            
        choice = choices[0]
        message = choice.get("message", {})
        common_header = {
            "id": openai_response.get("id"),
            "object": "chat.completion.chunk",
            "created": openai_response.get("created"),
            "model": openai_response.get("model"),
        }
        
        chunks = []
        
        # 1. ì—­í• (Role)ê³¼ ë³¸ë¬¸(Content) ì „ì†¡
        content_chunk = common_header.copy()
        content_chunk["choices"] = [{
            "index": 0,
            "delta": {
                "role": message.get("role"),
                "content": message.get("content")
            },
            "finish_reason": None
        }]
        chunks.append(f"data: {json.dumps(content_chunk, ensure_ascii=False)}\n\n")
        
        # 2. ë„êµ¬ í˜¸ì¶œ(Tool Calls) ì „ì†¡ (ìˆì„ ê²½ìš°ë§Œ)
        # 2. ë„êµ¬ í˜¸ì¶œ(Tool Calls) ì „ì†¡ (ìˆì„ ê²½ìš°ë§Œ)
        if message.get("tool_calls"):
            tool_chunk = common_header.copy()
            tool_chunk["choices"] = [{
                "index": 0,
                "delta": {
                    "tool_calls": message.get("tool_calls")
                },
                "finish_reason": "stop"
            }]
            chunks.append(f"data: {json.dumps(tool_chunk, ensure_ascii=False)}\n\n")
        else:
            # ë„êµ¬ê°€ ì—†ìœ¼ë©´ ë§ˆì§€ë§‰ì— finish_reason: stop ì¶”ê°€
            stop_chunk = common_header.copy()
            stop_chunk["choices"] = [{
                "index": 0,
                "delta": {},
                "finish_reason": choice.get("finish_reason") or "stop"
            }]
            chunks.append(f"data: {json.dumps(stop_chunk, ensure_ascii=False)}\n\n")
            
        return chunks
    
    @staticmethod
    def extract_tool_calls(
        response: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        ì‘ë‹µì—ì„œ ë„êµ¬ í˜¸ì¶œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            response: OpenAI í˜•ì‹ì˜ ì‘ë‹µ
            
        Returns:
            List[Dict]: ë„êµ¬ í˜¸ì¶œ ëª©ë¡
        """
        tool_calls = []
        
        try:
            choices = response.get("choices", [])
            if choices:
                message = choices[0].get("message", {})
                tool_calls = message.get("tool_calls", [])
                
                logger.info(f"[Adapter] ì¶”ì¶œëœ ë„êµ¬ í˜¸ì¶œ: {len(tool_calls)}ê±´")
                
        except Exception as e:
            logger.error(f"[Adapter] ë„êµ¬ í˜¸ì¶œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return tool_calls


class RequestValidator:
    """ìš”ì²­ ìœ íš¨ì„± ê²€ì¦ í´ë˜ìŠ¤"""
    
    @staticmethod
    def validate_chat_request(request: Dict[str, Any]) -> tuple[bool, str]:
        """
        ì±„íŒ… ìš”ì²­ì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.
        
        Args:
            request: ê²€ì¦í•  ìš”ì²­
            
        Returns:
            tuple[bool, str]: (ìœ íš¨ì„± ì—¬ë¶€, ì˜¤ë¥˜ ë©”ì‹œì§€)
        """
        # messages í•„ë“œ í™•ì¸
        if "messages" not in request:
            return False, "messages í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤"
        
        messages = request["messages"]
        if not isinstance(messages, list):
            return False, "messagesëŠ” ë°°ì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤"
        
        if len(messages) == 0:
            return False, "ìµœì†Œ í•˜ë‚˜ì˜ ë©”ì‹œì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤"
        
        # ê° ë©”ì‹œì§€ ê²€ì¦
        for i, msg in enumerate(messages):
            if not isinstance(msg, dict):
                return False, f"ë©”ì‹œì§€ {i}ëŠ” ê°ì²´ì—¬ì•¼ í•©ë‹ˆë‹¤"
            
            if "role" not in msg:
                return False, f"ë©”ì‹œì§€ {i}ì— roleì´ ì—†ìŠµë‹ˆë‹¤"
            
            if "content" not in msg and "tool_calls" not in msg:
                return False, f"ë©”ì‹œì§€ {i}ì— content ë˜ëŠ” tool_callsê°€ í•„ìš”í•©ë‹ˆë‹¤"
        
        logger.info("[Validator] ìš”ì²­ ìœ íš¨ì„± ê²€ì¦ í†µê³¼")
        return True, ""
    
    @staticmethod
    def validate_tools(tools: List[Dict[str, Any]]) -> tuple[bool, str]:
        """
        ë„êµ¬ ëª©ë¡ì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.
        
        Args:
            tools: ê²€ì¦í•  ë„êµ¬ ëª©ë¡
            
        Returns:
            tuple[bool, str]: (ìœ íš¨ì„± ì—¬ë¶€, ì˜¤ë¥˜ ë©”ì‹œì§€)
        """
        if not isinstance(tools, list):
            return False, "toolsëŠ” ë°°ì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤"
        
        for i, tool in enumerate(tools):
            if "type" not in tool:
                return False, f"ë„êµ¬ {i}ì— typeì´ ì—†ìŠµë‹ˆë‹¤"
            
            if tool["type"] == "function":
                if "function" not in tool:
                    return False, f"ë„êµ¬ {i}ì— function ì •ì˜ê°€ ì—†ìŠµë‹ˆë‹¤"
                
                func = tool["function"]
                if "name" not in func:
                    return False, f"ë„êµ¬ {i}ì— í•¨ìˆ˜ ì´ë¦„ì´ ì—†ìŠµë‹ˆë‹¤"
        
        logger.info(f"[Validator] ë„êµ¬ ìœ íš¨ì„± ê²€ì¦ í†µê³¼: {len(tools)}ê°œ")
        return True, ""
