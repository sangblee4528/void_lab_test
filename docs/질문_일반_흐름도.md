# ì¼ë°˜ ì§ˆë¬¸ ì²˜ë¦¬ íë¦„ë„

"ì•ˆë…•í•˜ì„¸ìš”" ê°™ì€ ì¼ë°˜ ì§ˆë¬¸ì´ Void IDEì—ì„œ ìµœì¢… ì‘ë‹µê¹Œì§€ ì²˜ë¦¬ë˜ëŠ” ì „ì²´ íë¦„ì„ ìƒì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.

## ì „ì²´ íë¦„ ë‹¤ì´ì–´ê·¸ë¨

```mermaid
sequenceDiagram
    participant Void as Void IDE<br/>(ì±„íŒ…ì°½)
    participant Agent as agent_proxy_server.py<br/>(127.0.0.1:8001)
    participant LLM as Ollama/vLLM<br/>(127.0.0.1:11434)

    Void->>Agent: POST /v1/chat/completions<br/>{"messages": [{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}]}
    
    Note over Agent: chat_completions() í•¨ìˆ˜ ì‹¤í–‰
    Agent->>Agent: MCPì—ì„œ ë„êµ¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°<br/>(toolsê°€ ì—†ìœ¼ë©´)
    
    Agent->>LLM: POST /v1/chat/completions<br/>+ tools ëª©ë¡ í¬í•¨
    Note over LLM: ëª¨ë¸ì´ ì¼ë°˜ ì‘ë‹µ ìƒì„±<br/>(ë„êµ¬ í˜¸ì¶œ ì—†ìŒ)
    LLM-->>Agent: {"content": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"}
    
    Note over Agent: tool_callsê°€ ì—†ìŒ<br/>â†’ ìµœì¢… ì‘ë‹µìœ¼ë¡œ íŒë‹¨
    Agent->>Agent: format_to_openai_response()<br/>OpenAI í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    Agent->>Agent: generate_pseudo_stream()<br/>3ê°œ ì²­í¬ë¡œ ë¶„í• 
    
    Agent-->>Void: SSE Stream:<br/>1. role: assistant<br/>2. content: "ì•ˆë…•í•˜ì„¸ìš”!..."<br/>3. finish_reason: stop
    
    Note over Void: ì±„íŒ…ì°½ì— ì‘ë‹µ í‘œì‹œ
```

## ìƒì„¸ ë‹¨ê³„ë³„ íë¦„

### 1ï¸âƒ£ Void IDE â†’ Agent Proxy Server

**íŒŒì¼**: `agent_proxy/agent_proxy_server.py`  
**í•¨ìˆ˜**: `chat_completions()` (118í–‰)

```python
@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest):
    request_id = datetime.now().strftime("%H%M%S")
    logger.info(f"ğŸ“¥ [Agent-{request_id}] ìƒˆ ìš”ì²­ ìˆ˜ì‹ : {request.messages[-1].content}")
    
    current_messages = [msg.model_dump(exclude_none=True) for msg in request.messages]
```

**ìš”ì²­ ë°ì´í„°**:
```json
{
  "messages": [
    {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}
  ],
  "stream": true
}
```

---

### 2ï¸âƒ£ Agent Proxy â†’ MCP Server (ë„êµ¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°)

**íŒŒì¼**: `agent_proxy/agent_proxy_server.py`  
**í•¨ìˆ˜**: `chat_completions()` (130-152í–‰)

```python
# ë„êµ¬ ìë™ ê²€ìƒ‰ (ìš”ì²­ì— ì—†ìœ¼ë©´ MCP ì„œë²„ì—ì„œ ê°€ì ¸ì˜´)
tools = request.tools
if not tools:
    logger.info(f"ğŸ” [Agent-{request_id}] MCP ì„œë²„ì—ì„œ ë„êµ¬ ëª©ë¡ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
    mcp_tools_resp = await httpx.AsyncClient().get(f"{config['mcp']['host']}/tools")
    mcp_tool_defs = mcp_tools_resp.json().get("tools", [])
    
    # MCP í˜•ì‹ì„ OpenAI/Ollama ë„êµ¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    tools = [
        {
            "type": "function",
            "function": {
                "name": t["name"],
                "description": t["description"],
                "parameters": t["inputSchema"]
            }
        }
        for t in mcp_tool_defs
    ]
```

**ìš”ì²­**: `GET http://127.0.0.1:3000/tools`  
**ì‘ë‹µ**: 4ê°œ ë„êµ¬ ì •ì˜ (search_docs, get_employee_info, get_all_employees, calculate_vacation_days)

> **ğŸ’¡ ì¤‘ìš”**: ì¼ë°˜ ì§ˆë¬¸ì´ì–´ë„ ë„êµ¬ ëª©ë¡ì€ í•­ìƒ LLMì—ê²Œ ì „ë‹¬ë©ë‹ˆë‹¤. ëª¨ë¸ì´ "ì´ ì§ˆë¬¸ì—ëŠ” ë„êµ¬ê°€ í•„ìš” ì—†ë‹¤"ê³  íŒë‹¨í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

---

### 3ï¸âƒ£ Agent Proxy â†’ LLM (ì§ˆë¬¸ ì „ë‹¬)

**íŒŒì¼**: `agent_proxy/agent_proxy_server.py`  
**í•¨ìˆ˜**: ììœ¨ ì‹¤í–‰ ë£¨í”„ (159-227í–‰) â†’ `call_llm()` (277í–‰)

```python
# ğŸ”„ Autonomous Agent Loop ì‹œì‘
for i in range(max_iterations):
    logger.info(f"ğŸ”„ [Agent-{request_id}] ë°˜ë³µ {i+1}ë‹¨ê³„ ì‹¤í–‰ ì¤‘...")
    
    # LLMì—ê²Œ í˜„ì¬ê¹Œì§€ì˜ ëŒ€í™” ì´ë ¥ ì „ë‹¬
    logger.info(f"ğŸ“¤ [Agent-{request_id}] [LLM REQ] LLMì—ê²Œ ë‹µë³€ ìš”ì²­ ì¤‘...")
    full_ollama_resp = await call_llm(current_messages, tools)
```

**`call_llm()` í•¨ìˆ˜**:
```python
async def call_llm(messages: List[Dict], tools: Optional[List] = None):
    async with httpx.AsyncClient(timeout=config["llm"]["timeout"]) as client:
        url = f"{config['llm']['base_url']}/chat/completions"
        headers = {}
        api_key = str(config["llm"].get("api_key", "")).strip()
        if api_key and api_key.lower() != "not-needed":
            headers["Authorization"] = f"Bearer {api_key}"

        payload = {
            "model": config["llm"]["model"],
            "messages": messages,
            "stream": False,
            "temperature": 0
        }
        if tools:
            payload["tools"] = tools
            
        resp = await client.post(url, json=payload, headers=headers)
        resp.raise_for_status()
        return resp.json()
```

**ìš”ì²­**: `POST http://127.0.0.1:11434/v1/chat/completions`

**ìš”ì²­ í˜ì´ë¡œë“œ**:
```json
{
  "model": "qwen2.5-coder:7b",
  "messages": [
    {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}
  ],
  "tools": [
    {"type": "function", "function": {"name": "search_docs", ...}},
    {"type": "function", "function": {"name": "get_employee_info", ...}},
    ...
  ],
  "stream": false,
  "temperature": 0
}
```

---

### 4ï¸âƒ£ LLM â†’ Agent Proxy (ì¼ë°˜ ì‘ë‹µ)

**LLM ì‘ë‹µ**:
```json
{
  "id": "chatcmpl-190",
  "object": "chat.completion",
  "created": 1767701995,
  "model": "qwen2.5-coder:7b",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
      },
      "finish_reason": "stop"
    }
  ]
}
```

> **ğŸ’¡ í•µì‹¬**: `tool_calls` í•„ë“œê°€ **ì—†ìŠµë‹ˆë‹¤**. ëª¨ë¸ì´ "ì´ ì§ˆë¬¸ì€ ë„êµ¬ ì—†ì´ ë‹µë³€ ê°€ëŠ¥"í•˜ë‹¤ê³  íŒë‹¨í–ˆìŠµë‹ˆë‹¤.

---

### 5ï¸âƒ£ Agent Proxy (ì‘ë‹µ ì²˜ë¦¬)

**íŒŒì¼**: `agent_proxy/agent_proxy_server.py`  
**í•¨ìˆ˜**: ììœ¨ ì‹¤í–‰ ë£¨í”„ (171-227í–‰)

```python
logger.info(f"ğŸ“¥ [Agent-{request_id}] [LLM RESP] ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")

choice = full_ollama_resp.get("choices", [{}])[0]
message = choice.get("message", {})
tool_calls = message.get("tool_calls", [])  # â† None ë˜ëŠ” []
content = message.get("content", "")

# tool_callsê°€ ì—†ìœ¼ë©´ ìµœì¢… ì‘ë‹µìœ¼ë¡œ íŒë‹¨
if not tool_calls:
    logger.info(f"âœ… [Agent-{request_id}] ìµœì¢… ì‘ë‹µ ë„ë‹¬")
    final_resp = format_to_openai_response(full_ollama_resp)
    
    if request.stream:
        logger.info(f"ğŸ“¡ [Agent-{request_id}] ìŠ¤íŠ¸ë¦¬ë° í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜")
        return StreamingResponse(
            generate_pseudo_stream(final_resp),
            media_type="text/event-stream"
        )
    else:
        return final_resp
```

---

### 6ï¸âƒ£ OpenAI í˜•ì‹ ë³€í™˜

**íŒŒì¼**: `agent_proxy/agent_proxy_server.py`  
**í•¨ìˆ˜**: `format_to_openai_response()` (355í–‰)

```python
def format_to_openai_response(ollama_resp: Dict):
    """Ollama ì‘ë‹µ í˜•ì‹ì„ OpenAI ê·œê²©ìœ¼ë¡œ ë³€í™˜"""
    choice = ollama_resp.get("choices", [{}])[0]
    message = choice.get("message", {})
    return {
        "id": "agent-" + datetime.now().strftime("%Y%m%d%H%M%S"),
        "object": "chat.completion",
        "created": int(datetime.now().timestamp()),
        "model": ollama_resp.get("model", config["llm"]["model"]),
        "choices": [
            {
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": message.get("content", "")
                },
                "finish_reason": "stop"
            }
        ],
        "usage": {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0
        }
    }
```

**ë³€í™˜ ê²°ê³¼**:
```json
{
  "id": "agent-20260106211900",
  "object": "chat.completion",
  "created": 1767701940,
  "model": "qwen2.5-coder:7b",
  "choices": [{
    "index": 0,
    "message": {
      "role": "assistant",
      "content": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
    },
    "finish_reason": "stop"
  }],
  "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
}
```

---

### 7ï¸âƒ£ ìŠ¤íŠ¸ë¦¬ë° í˜•ì‹ ë³€í™˜

**íŒŒì¼**: `agent_proxy/agent_proxy_server.py`  
**í•¨ìˆ˜**: `generate_pseudo_stream()` (304í–‰)

```python
def generate_pseudo_stream(final_resp: Dict):
    """ì¼ë°˜ ì‘ë‹µì„ SSE ìŠ¤íŠ¸ë¦¼ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    # ì²« ë²ˆì§¸ ì²­í¬: roleë§Œ ì „ì†¡
    chunk1 = {
        "id": final_resp["id"],
        "object": "chat.completion.chunk",
        "created": final_resp["created"],
        "model": final_resp["model"],
        "choices": [
            {
                "index": 0,
                "delta": {"role": "assistant"},
                "finish_reason": None
            }
        ]
    }
    yield f"data: {json.dumps(chunk1, ensure_ascii=False)}\\n\\n"
    
    # ë‘ ë²ˆì§¸ ì²­í¬: content ì „ì†¡
    chunk2 = {
        "id": final_resp["id"],
        "object": "chat.completion.chunk",
        "created": final_resp["created"],
        "model": final_resp["model"],
        "choices": [
            {
                "index": 0,
                "delta": {"content": final_resp["choices"][0]["message"]["content"]},
                "finish_reason": None
            }
        ]
    }
    yield f"data: {json.dumps(chunk2, ensure_ascii=False)}\\n\\n"
    
    # ì„¸ ë²ˆì§¸ ì²­í¬: finish_reason
    chunk3 = {
        "id": final_resp["id"],
        "object": "chat.completion.chunk",
        "created": final_resp["created"],
        "model": final_resp["model"],
        "choices": [
            {
                "index": 0,
                "delta": {},
                "finish_reason": "stop"
            }
        ]
    }
    yield f"data: {json.dumps(chunk3, ensure_ascii=False)}\\n\\n"
    yield "data: [DONE]\\n\\n"
```

---

### 8ï¸âƒ£ Agent Proxy â†’ Void IDE (SSE ìŠ¤íŠ¸ë¦¼ ì „ì†¡)

**SSE ìŠ¤íŠ¸ë¦¼**:
```
data: {"id":"agent-20260106211900","object":"chat.completion.chunk","created":1767701940,"model":"qwen2.5-coder:7b","choices":[{"index":0,"delta":{"role":"assistant"},"finish_reason":null}]}

data: {"id":"agent-20260106211900","object":"chat.completion.chunk","created":1767701940,"model":"qwen2.5-coder:7b","choices":[{"index":0,"delta":{"content":"ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"},"finish_reason":null}]}

data: {"id":"agent-20260106211900","object":"chat.completion.chunk","created":1767701940,"model":"qwen2.5-coder:7b","choices":[{"index":0,"delta":{},"finish_reason":"stop"}]}

data: [DONE]
```

---

### 9ï¸âƒ£ Void IDE (ìµœì¢… ë Œë”ë§)

Void IDEê°€ SSE ìŠ¤íŠ¸ë¦¼ì„ ìˆ˜ì‹ í•˜ì—¬ ì±„íŒ…ì°½ì— ë©”ì‹œì§€ë¥¼ ì ì§„ì ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.

**ìµœì¢… í™”ë©´**:
```
ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?
```

---

## MCP ì§ˆë¬¸ íë¦„ê³¼ì˜ ì°¨ì´ì 

| ë‹¨ê³„ | ì¼ë°˜ ì§ˆë¬¸ | MCP ì§ˆë¬¸ (ë„êµ¬ ì‚¬ìš©) |
|------|----------|---------------------|
| **LLM í˜¸ì¶œ íšŸìˆ˜** | **1íšŒ** | **2íšŒ ì´ìƒ** (ë„êµ¬ ê²°ê³¼ ë°˜ì˜) |
| **MCP ì„œë²„ í˜¸ì¶œ** | âŒ ì—†ìŒ | âœ… ìˆìŒ (ë„êµ¬ ì‹¤í–‰) |
| **ììœ¨ ë£¨í”„ ë°˜ë³µ** | 1íšŒë§Œ (ì¦‰ì‹œ ì¢…ë£Œ) | ì—¬ëŸ¬ ë²ˆ (ë„êµ¬ ê²°ê³¼ ë°˜ì˜) |
| **ì‘ë‹µ ì‹œê°„** | ë¹ ë¦„ (~2ì´ˆ) | ëŠë¦¼ (~5-10ì´ˆ) |

---

## í•µì‹¬ í¬ì¸íŠ¸ ì •ë¦¬

### ğŸš€ ë¹ ë¥¸ ì‘ë‹µ ê²½ë¡œ
ì¼ë°˜ ì§ˆë¬¸ì€ **MCP ì„œë²„ë¥¼ ê±°ì¹˜ì§€ ì•Šê³ ** Agent Proxy â†” LLMë§Œ ì™•ë³µí•˜ë¯€ë¡œ ë§¤ìš° ë¹ ë¦…ë‹ˆë‹¤.

### ğŸ”„ ììœ¨ ë£¨í”„ì˜ ì¡°ê¸° ì¢…ë£Œ
`tool_calls`ê°€ ì—†ìœ¼ë©´ ë£¨í”„ê°€ **ì²« ë²ˆì§¸ ë°˜ë³µì—ì„œ ì¦‰ì‹œ ì¢…ë£Œ**ë©ë‹ˆë‹¤.

### ğŸ“¡ ìŠ¤íŠ¸ë¦¬ë° ì¼ê´€ì„±
ì¼ë°˜ ì§ˆë¬¸ë„ ë„êµ¬ ì§ˆë¬¸ê³¼ **ë™ì¼í•œ 3ì²­í¬ ìŠ¤íŠ¸ë¦¬ë° í˜•ì‹**ì„ ì‚¬ìš©í•˜ì—¬ Void IDEì˜ ë Œë”ë§ ë¡œì§ì„ ë‹¨ìˆœí™”í•©ë‹ˆë‹¤.

### ğŸ› ï¸ ë„êµ¬ ëª©ë¡ì€ í•­ìƒ ì „ë‹¬
ì¼ë°˜ ì§ˆë¬¸ì´ì–´ë„ LLMì—ê²Œ **ë„êµ¬ ëª©ë¡ì„ í•¨ê»˜ ì „ë‹¬**í•©ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ "í•„ìš”í•˜ë©´ ë„êµ¬ë¥¼ ì“¸ ìˆ˜ ìˆë‹¤"ëŠ” ì„ íƒê¶Œì„ ì£¼ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.
