"""
# agent_native_loop_server.py - ììœ¨ì ìœ¼ë¡œ ë„êµ¬ë¥¼ ì‹¤í–‰í•˜ëŠ” ëŠ¥ë™ì  ëŒ€ë¦¬ì¸ ì„œë²„ (Native ë²„ì „)

LLMì´ ë„êµ¬ í˜¸ì¶œì„ ìš”ì²­í•˜ë©´, í´ë¼ì´ì–¸íŠ¸(Void)ì—ê²Œ ë°˜í™˜í•˜ê¸° ì „ì— 
ì§ì ‘ MCP ì„œë²„ì™€ í†µì‹ í•˜ì—¬ ë„êµ¬ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ LLMì—ê²Œ ë‹¤ì‹œ ì „ë‹¬í•©ë‹ˆë‹¤.
ìµœì¢… ë‹µë³€ì´ ë‚˜ì˜¬ ë•Œê¹Œì§€ ì´ ê³¼ì •ì„ ë°˜ë³µí•©ë‹ˆë‹¤.
"""

import asyncio
import json
import logging
import sys
import sqlite3
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional

import httpx
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
from pydantic import BaseModel

# ë¡œì»¬ ëª¨ë“ˆ
# ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ë¥¼ ê²½ë¡œì— ì¶”ê°€í•˜ì—¬ ì–´ë””ì„œ ì‹¤í–‰í•˜ë“  native_toolsë¥¼ ì°¾ì„ ìˆ˜ ìˆê²Œ í•¨
sys.path.append(str(Path(__file__).parent))
from native_loop_tools import NATIVE_TOOL_DEFS, NATIVE_TOOL_REGISTRY

# ì„¤ì • ë¡œë“œ
CONFIG_PATH = (Path(__file__).parent / "agent_native_loop_config" / "agent_native_loop_config.json").resolve()

def load_config():
    with open(CONFIG_PATH, encoding="utf-8") as f:
        config = json.load(f)
        
    # í”„ë¡œíŒŒì¼ ì§€ì›: active_profileì´ ìˆìœ¼ë©´ í•´ë‹¹ ì„¤ì •ì„ llm ì„¹ì…˜ìœ¼ë¡œ ë³µì‚¬
    if "active_profile" in config and "llm_profiles" in config:
        active = config["active_profile"]
        if active in config["llm_profiles"]:
            config["llm"] = config["llm_profiles"][active]
            
    return config

config = load_config()

# ë¡œê¹… ì„¤ì •
LOG_FILE = (Path(__file__).parent / "agent_native_loop.log").resolve()
logging.basicConfig(
    level=getattr(logging, config["logging"]["level"]),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(LOG_FILE, encoding="utf-8")
    ]
)
logger = logging.getLogger("agent_native_loop")
# mcp_client ë¡œê±°ë„ ê°™ì€ í•¸ë“¤ëŸ¬ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì • (ìƒì†)
logging.getLogger("mcp_loop_client").setLevel(getattr(logging, config["logging"]["level"]))

# DB ê²½ë¡œ í•´ê²° (ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜)
DB_RELATIVE_PATH = config.get("database", {}).get("path", "../db/agent_native_loop_data.db")
DB_PATH = (Path(__file__).parent / "agent_native_loop_config" / DB_RELATIVE_PATH).resolve()

# MCP í´ë¼ì´ì–¸íŠ¸ ì œê±° (ë¡œì»¬ ë„êµ¬ ì‚¬ìš©)
# mcp_client = McpSseClient(config["mcp"]["host"], db_path=DB_PATH)

def save_agent_log(request_id: str, message: str, details: Optional[str] = None):
    """DBì— ì—ì´ì „íŠ¸ í™œë™ ë¡œê·¸ ì €ì¥"""
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute(
            "INSERT INTO agent_logs (request_id, message, details) VALUES (?, ?, ?)",
            (request_id, message, details)
        )
        conn.commit()
        conn.close()
    except Exception as e:
        logger.error(f"âš ï¸ DB ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")

async def ask_terminal_approval(func_name: str, args: Dict) -> bool:
    """
    í„°ë¯¸ë„ì—ì„œ ë„êµ¬ ì‹¤í–‰ ìŠ¹ì¸ì„ ìš”ì²­í•©ë‹ˆë‹¤.
    y/Y/yes/Yes ì…ë ¥ ì‹œ True, ê·¸ ì™¸ëŠ” False ë°˜í™˜
    """
    print("\n" + "="*60)
    print(f"ğŸ”§ ë„êµ¬ ì‹¤í–‰ ìŠ¹ì¸ ìš”ì²­")
    print(f"   ë„êµ¬: {func_name}")
    print(f"   ì¸ì: {json.dumps(args, ensure_ascii=False, indent=2)}")
    print("="*60)
    
    # async ë°©ì‹ìœ¼ë¡œ input() í˜¸ì¶œ (ì´ë²¤íŠ¸ ë£¨í”„ ë¸”ë¡œí‚¹ ë°©ì§€)
    loop = asyncio.get_event_loop()
    user_input = await loop.run_in_executor(None, lambda: input("ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): "))
    
    approved = user_input.strip().lower() in ['y', 'yes', 'ì˜ˆ', 'ã…›']
    if approved:
        print("âœ… ìŠ¹ì¸ë¨ - ë„êµ¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.\n")
    else:
        print("âŒ ê±°ì ˆë¨ - ë„êµ¬ ì‹¤í–‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.\n")
    
    return approved

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    logger.info("Agent Native Loop Server starting (Truly Native Mode)...")
    logger.info(f"{len(NATIVE_TOOL_DEFS)} native tools loaded")
    yield
    logger.info("Agent Native Loop Server stopped")

app = FastAPI(title="Void Lab Test - Active Agent Native Loop", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"])

# ìš”ì²­ ëª¨ë¸
class ChatMessage(BaseModel):
    role: str
    content: Optional[str] = None
    tool_calls: Optional[List[Dict[str, Any]]] = None
    tool_call_id: Optional[str] = None
    name: Optional[str] = None

class ChatRequest(BaseModel):
    model: Optional[str] = None
    messages: List[ChatMessage]
    tools: Optional[List[Dict[str, Any]]] = None
    stream: bool = False

@app.get("/")
async def root():
    """ì„œë²„ ìƒíƒœ ë° LLM ì—°ê²° í™•ì¸ìš© ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    llm_connected = False
    llm_info = {}
    try:
        async with httpx.AsyncClient(timeout=2.0) as client:
            resp = await client.get(f"{config['llm']['base_url']}/models")
            llm_connected = resp.status_code == 200
            llm_info = resp.json() if llm_connected else {"error": resp.text}
    except Exception as e:
        llm_info = {"error": str(e)}

    return {
        "status": "online",
        "agent": "Agent Native Loop Server",
        "version": "1.1.0",
        "llm_connection": {
            "status": "connected" if llm_connected else "disconnected",
            "base_url": config['llm']['base_url'],
            "model": config['llm']['model'],
            "details": llm_info
        },
        "endpoints": {
            "models": "/v1/models",
            "chat": "/v1/chat/completions (POST only)"
        }
    }

@app.get("/v1/models")
async def list_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜ (Void IDE ì´ˆê¸°í™” ëŒ€ì‘)"""
    return {
        "object": "list",
        "data": [
            {
                "id": config["llm"]["model"],
                "object": "model",
                "created": int(datetime.now().timestamp()),
                "owned_by": config["llm"]["provider"]
            }
        ]
    }

@app.get("/v1/chat/completions")
async def chat_completions_get():
    """GET ìš”ì²­ ì‹œ ì•ˆë‚´ ë©”ì‹œì§€ ë°˜í™˜"""
    return {
        "error": "Method Not Allowed",
        "message": "ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” POST ìš”ì²­ë§Œ ì§€ì›í•©ë‹ˆë‹¤. Void IDEë‚˜ API í´ë¼ì´ì–¸íŠ¸ ì„¤ì •ì—ì„œ POST ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ê³  ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.",
        "hint": "OpenAI í˜¸í™˜ API ê·œê²©ì€ ì±„íŒ… ì™„ë£Œë¥¼ ìœ„í•´ POST /v1/chat/completionsë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
    }

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest):
    """
    ììœ¨ ì‹¤í–‰ ë£¨í”„ë¥¼ í¬í•¨í•œ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸
    """
    request_id = datetime.now().strftime("%H%M%S")
    logger.info(f"[Agent-{request_id}] New request received: {request.messages[-1].content}")
    save_agent_log(request_id, "Request Received", request.messages[-1].content)
    
    try:
        current_messages = [msg.model_dump(exclude_none=True) for msg in request.messages]
        tools = request.tools if request.tools else NATIVE_TOOL_DEFS
        
        max_iterations = 5
        iteration = 0
        final_response = None
        
        while iteration < max_iterations:
            iteration += 1
            logger.info(f"[Agent-{request_id}] [LLM REQ] Loop {iteration}/{max_iterations}")
            
            # [HITL Feedback Loop Injection]
            last_msg = current_messages[-1] if current_messages else None
            if last_msg and last_msg.get("role") == "tool":
                try:
                    content_obj = json.loads(last_msg.get("content", "{}"))
                    if isinstance(content_obj, dict) and not content_obj.get("success", True):
                        error_msg = content_obj.get("error", "Unknown error")
                        feedback_guidance = f"\n\n[SYSTEM FEEDBACK]\në„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}\nì›ì¸ì„ ë¶„ì„í•˜ê³  í•„ìš”í•œ ê²½ìš° ìˆ˜ì •ëœ ì¸ìë¡œ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ë‹¤ë¥¸ ë°©ë²•ì„ ì°¾ì•„ì£¼ì„¸ìš”."
                        last_msg["content"] = last_msg.get("content", "") + feedback_guidance
                except Exception:
                    pass

            # LLM í˜¸ì¶œ
            full_ollama_resp = await call_llm(current_messages, tools)
            choice = full_ollama_resp.get("choices", [{}])[0]
            assistant_msg = choice.get("message", {})
            current_messages.append(assistant_msg)
            
            # [Tool Call Detection]
            detected_tool_calls = assistant_msg.get("tool_calls", [])
            logger.debug(f"[Agent-{request_id}] Initial tool_calls: {detected_tool_calls}")
            if not isinstance(detected_tool_calls, list):
                detected_tool_calls = []
                
            # contentì—ì„œ ì¶”ê°€ë¡œ ì°¾ê¸°
            if assistant_msg.get("content"):
                content = assistant_msg["content"].strip()
                logger.debug(f"[Agent-{request_id}] Checking content for tools: {content[:100]}...")
                try:
                    # 1. ```json ... ``` ë¸”ë¡ ì°¾ê¸°
                    json_matches = re.findall(r"```(?:json)?\s*(\{.*?\})\s*```", content, re.DOTALL)
                    
                    # 2. ë¸”ë¡ì´ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ ì „ì²´ì—ì„œ { } ìŒ ì°¾ê¸° (ë” ê²¬ê³ í•œ ë°©ì‹)
                    if not json_matches:
                        start_idx = content.find("{")
                        end_idx = content.rfind("}")
                        if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                            # ê°€ì¥ ë°”ê¹¥ìª½ì˜ { } ë¸”ë¡ í•˜ë‚˜ë¥¼ ì¶”ì¶œ
                            json_matches = [content[start_idx:end_idx+1]]
                        else:
                            json_matches = []
                    
                    logger.debug(f"[Agent-{request_id}] Found {len(json_matches)} potential JSON blocks")
                    for match in json_matches:
                        try:
                            potential_tool = json.loads(match)
                            logger.debug(f"[Agent-{request_id}] Parsed JSON: {list(potential_tool.keys())}")
                            # nameê³¼ arguments(ë˜ëŠ” args)ê°€ ìˆìœ¼ë©´ ë„êµ¬ í˜¸ì¶œë¡œ ê°„ì£¼
                            if isinstance(potential_tool, dict) and "name" in potential_tool and ("arguments" in potential_tool or "args" in potential_tool):
                                # ì´ë¯¸ ë°œê²¬ëœ tool_callsì— ë™ì¼í•œ nameì´ ìˆëŠ”ì§€ í™•ì¸ (ì¤‘ë³µ ë°©ì§€)
                                if not any(tc.get("function", {}).get("name") == potential_tool["name"] for tc in detected_tool_calls):
                                    logger.info(f"[Agent-{request_id}] Tool call '{potential_tool['name']}' detected in content")
                                    detected_tool_calls.append({
                                        "id": f"call_{datetime.now().strftime('%H%M%S%f')}",
                                        "type": "function",
                                        "function": {
                                            "name": potential_tool["name"],
                                            "arguments": potential_tool.get("arguments") or potential_tool.get("args") or {}
                                        }
                                    })
                        except json.JSONDecodeError as je:
                            logger.debug(f"[Agent-{request_id}] JSONDecodeError for block: {je}")
                            continue
                except Exception as e:
                    logger.debug(f"[Agent-{request_id}] Content parsing error: {e}")

            if not detected_tool_calls:
                logger.info(f"[Agent-{request_id}] Final response received (Loop finished)")
                final_response = full_ollama_resp
                break
            
            # tool_calls ì—…ë°ì´íŠ¸ (ë£¨í”„ ì§„í–‰ì„ ìœ„í•´)
            assistant_msg["tool_calls"] = detected_tool_calls
            tool_calls = detected_tool_calls
            
            # ë„êµ¬ ì‹¤í–‰ (ìŠ¹ì¸ í•„ìš”)
            logger.info(f"[Agent-{request_id}] Starting {len(tool_calls)} tools (approval required)")
            rejected = False
            
            for tc in tool_calls:
                func_name = tc["function"]["name"]
                args = tc["function"]["arguments"]
                if isinstance(args, str):
                    try:
                        args = json.loads(args)
                    except:
                        pass
                
                logger.info(f"[Agent-{request_id}] Tool call: {func_name}({args})")
                
                # ğŸ”’ í„°ë¯¸ë„ ìŠ¹ì¸ ìš”ì²­
                approved = await ask_terminal_approval(func_name, args if isinstance(args, dict) else {})
                
                if not approved:
                    rejected = True
                    result = {"success": False, "error": "ì‚¬ìš©ìê°€ ë„êµ¬ ì‹¤í–‰ì„ ê±°ì ˆí–ˆìŠµë‹ˆë‹¤."}
                    save_agent_log(request_id, f"Tool Rejected: {func_name}", "User rejected")
                elif func_name in NATIVE_TOOL_REGISTRY:
                    try:
                        if isinstance(args, dict):
                            result = NATIVE_TOOL_REGISTRY[func_name](**args)
                        else:
                            result = NATIVE_TOOL_REGISTRY[func_name]()
                    except Exception as e:
                        result = {"success": False, "error": str(e)}
                else:
                    result = {"success": False, "error": f"Tool '{func_name}' not found"}
                
                tool_msg = {
                    "role": "tool",
                    "tool_call_id": tc.get("id", "none"),
                    "name": func_name,
                    "content": json.dumps(result, ensure_ascii=False)
                }
                current_messages.append(tool_msg)
                save_agent_log(request_id, f"Tool Executed: {func_name}", json.dumps(result, ensure_ascii=False))
                
                # ê±°ì ˆ ì‹œ ë£¨í”„ ì¤‘ë‹¨
                if rejected:
                    logger.info(f"[Agent-{request_id}] User rejected tool execution. Stopping loop.")
                    break
            
            # ê±°ì ˆ ì‹œ ì „ì²´ ë£¨í”„ ì¢…ë£Œ
            if rejected:
                final_response = {
                    "id": "agent-" + datetime.now().strftime("%Y%m%d%H%M%S"),
                    "object": "chat.completion",
                    "created": int(datetime.now().timestamp()),
                    "model": config["llm"]["model"],
                    "choices": [{
                        "index": 0,
                        "message": {"role": "assistant", "content": "ì‚¬ìš©ìê°€ ë„êµ¬ ì‹¤í–‰ì„ ê±°ì ˆí•˜ì—¬ ì‘ì—…ì„ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤."},
                        "finish_reason": "stop"
                    }]
                }
                break

        if not final_response:
            # ìµœëŒ€ íšŸìˆ˜ ì´ˆê³¼ ì‹œ ë§ˆì§€ë§‰ ì‘ë‹µ ë°˜í™˜
            final_response = full_ollama_resp

        # ê²°ê³¼ ë°˜í™˜ (ìŠ¤íŠ¸ë¦¬ë° ì—¬ë¶€ì— ë”°ë¼)
        if request.stream:
            return StreamingResponse(
                generate_pseudo_stream_hitl(final_response),
                media_type="text/event-stream"
            )
        else:
            return final_response
        
    except Exception as e:
        logger.error(f"âŒ [Agent-{request_id}] ì²˜ë¦¬ ì¤‘ ì¹˜ëª…ì  ì—ëŸ¬: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

async def call_llm(messages: List[Dict], tools: Optional[List] = None):
    """LLM(Ollama, vLLM, OpenAI ë“±)ì˜ OpenAI í˜¸í™˜ API í˜¸ì¶œ"""
    async with httpx.AsyncClient(timeout=config["llm"]["timeout"]) as client:
        url = f"{config['llm']['base_url']}/chat/completions"
        headers = {"Content-Type": "application/json"}
        api_key = str(config["llm"].get("api_key", "")).strip()
        if api_key and api_key.lower() != "not-needed":
            headers["Authorization"] = f"Bearer {api_key}"

        payload = {
            "model": config["llm"]["model"],
            "messages": messages,
            "stream": False,
            "temperature": 0
        }
        if tools:
            payload["tools"] = tools
            
        logger.debug(f"ğŸ“¡ [LLM TX] Payload:\n{json.dumps(payload, ensure_ascii=False, indent=2)}")
        
        try:
            resp = await client.post(url, json=payload, headers=headers)
            resp.raise_for_status()
            return resp.json()
        except httpx.RemoteProtocolError as e:
            logger.error(f"âŒ LLM ì„œë²„(Ollama)ê°€ ì—°ê²°ì„ ê°•ì œë¡œ ëŠì—ˆìŠµë‹ˆë‹¤: {e}")
            raise HTTPException(status_code=500, detail=f"LLM Connection Reset: {str(e)}")
        except Exception as e:
            logger.error(f"âŒ LLM í˜¸ì¶œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
            raise

def generate_pseudo_stream_hitl(full_resp: Dict):
    """
    LLM ì‘ë‹µì„ OpenAI í˜¸í™˜ SSE ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì „ì†¡í•©ë‹ˆë‹¤.
    HITL ëª¨ë“œì—ì„œëŠ” tool_callsê°€ í¬í•¨ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì´ë¥¼ ê³ ë ¤í•©ë‹ˆë‹¤.
    """
    choice = full_resp.get("choices", [{}])[0]
    msg = choice.get("message", {})
    content = msg.get("content", "")
    tool_calls = msg.get("tool_calls", [])
    
    resp_id = full_resp.get("id", "hitl-" + datetime.now().strftime("%Y%m%d%H%M%S"))
    model_name = full_resp.get("model", config["llm"]["model"])
    created_time = full_resp.get("created", int(datetime.now().timestamp()))

    # 1. Start chunk (role)
    chunk = {
        "id": resp_id, "object": "chat.completion.chunk", "created": created_time, "model": model_name,
        "choices": [{"index": 0, "delta": {"role": "assistant"}, "finish_reason": None}]
    }
    yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"

    # 2. Content chunk (if any)
    if content:
        chunk = {
            "id": resp_id, "object": "chat.completion.chunk", "created": created_time, "model": model_name,
            "choices": [{"index": 0, "delta": {"content": content}, "finish_reason": None}]
        }
        yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"

    # 3. Tool Calls chunk (if any)
    if tool_calls:
        chunk = {
            "id": resp_id, "object": "chat.completion.chunk", "created": created_time, "model": model_name,
            "choices": [{"index": 0, "delta": {"tool_calls": tool_calls}, "finish_reason": None}]
        }
        yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"

    # 4. End chunk
    chunk = {
        "id": resp_id, "object": "chat.completion.chunk", "created": created_time, "model": model_name,
        "choices": [{"index": 0, "delta": {}, "finish_reason": choice.get("finish_reason", "stop")}]
    }
    yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
    yield "data: [DONE]\n\n"

def format_to_openai_response(ollama_resp: Dict):
    """Ollama ì‘ë‹µ í˜•ì‹ì„ OpenAI ê·œê²©ìœ¼ë¡œ ë³€í™˜"""
    choice = ollama_resp.get("choices", [{}])[0]
    message = choice.get("message", {})
    return {
        "id": "agent-" + datetime.now().strftime("%Y%m%d%H%M%S"),
        "object": "chat.completion",
        "created": int(datetime.now().timestamp()),
        "model": ollama_resp.get("model", config["llm"]["model"]),
        "choices": [
            {
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": message.get("content", "")
                },
                "finish_reason": "stop"
            }
        ],
        "usage": {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0
        }
    }

if __name__ == "__main__":
    import uvicorn
    import signal
    import sys
    
    def signal_handler(sig, frame):
        """Ctrl+C ë“± ì¢…ë£Œ ì‹œê·¸ë„ ì²˜ë¦¬"""
        print("\nğŸ›‘ ì¢…ë£Œ ì‹ í˜¸ ìˆ˜ì‹ . ì„œë²„ë¥¼ ì •ìƒ ì¢…ë£Œí•©ë‹ˆë‹¤...")
        sys.exit(0)
    
    # ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ë“±ë¡
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # uvicorn ì‹¤í–‰ (graceful shutdown ì„¤ì • í¬í•¨)
    config_uvicorn = uvicorn.Config(
        app,
        host=config["agent"]["host"],
        port=config["agent"]["port"],
        loop="asyncio",
        timeout_graceful_shutdown=5  # 5ì´ˆ ë‚´ graceful shutdown
    )
    server = uvicorn.Server(config_uvicorn)
    
    try:
        server.run()
    except KeyboardInterrupt:
        print("\nğŸ›‘ í‚¤ë³´ë“œ ì¸í„°ëŸ½íŠ¸. ì„œë²„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤...")
    finally:
        print("âœ… ì„œë²„ê°€ ì •ìƒ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. í¬íŠ¸ê°€ í•´ì œë˜ì—ˆìŠµë‹ˆë‹¤.")

