"""
# agent_native_loop_server.py - ììœ¨ì ìœ¼ë¡œ ë„êµ¬ë¥¼ ì‹¤í–‰í•˜ëŠ” ëŠ¥ë™ì  ëŒ€ë¦¬ì¸ ì„œë²„ (Native ë²„ì „)

LLMì´ ë„êµ¬ í˜¸ì¶œì„ ìš”ì²­í•˜ë©´, í´ë¼ì´ì–¸íŠ¸(Void)ì—ê²Œ ë°˜í™˜í•˜ê¸° ì „ì— 
ì§ì ‘ MCP ì„œë²„ì™€ í†µì‹ í•˜ì—¬ ë„êµ¬ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ LLMì—ê²Œ ë‹¤ì‹œ ì „ë‹¬í•©ë‹ˆë‹¤.
ìµœì¢… ë‹µë³€ì´ ë‚˜ì˜¬ ë•Œê¹Œì§€ ì´ ê³¼ì •ì„ ë°˜ë³µí•©ë‹ˆë‹¤.
"""

import json
import logging
import sys
import sqlite3
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional

import httpx
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
from pydantic import BaseModel

# ë¡œì»¬ ëª¨ë“ˆ
# ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ë¥¼ ê²½ë¡œì— ì¶”ê°€í•˜ì—¬ ì–´ë””ì„œ ì‹¤í–‰í•˜ë“  native_toolsë¥¼ ì°¾ì„ ìˆ˜ ìˆê²Œ í•¨
sys.path.append(str(Path(__file__).parent))
from native_loop_tools import NATIVE_TOOL_DEFS, NATIVE_TOOL_REGISTRY

# ì„¤ì • ë¡œë“œ
CONFIG_PATH = (Path(__file__).parent / "agent_native_loop_config" / "agent_native_loop_config.json").resolve()

def load_config():
    with open(CONFIG_PATH, encoding="utf-8") as f:
        config = json.load(f)
        
    # í”„ë¡œíŒŒì¼ ì§€ì›: active_profileì´ ìˆìœ¼ë©´ í•´ë‹¹ ì„¤ì •ì„ llm ì„¹ì…˜ìœ¼ë¡œ ë³µì‚¬
    if "active_profile" in config and "llm_profiles" in config:
        active = config["active_profile"]
        if active in config["llm_profiles"]:
            config["llm"] = config["llm_profiles"][active]
            
    return config

config = load_config()

# ë¡œê¹… ì„¤ì •
LOG_FILE = (Path(__file__).parent / "agent_native_loop.log").resolve()
logging.basicConfig(
    level=getattr(logging, config["logging"]["level"]),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(LOG_FILE, encoding="utf-8")
    ]
)
logger = logging.getLogger("agent_native_loop")
# mcp_client ë¡œê±°ë„ ê°™ì€ í•¸ë“¤ëŸ¬ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì • (ìƒì†)
logging.getLogger("mcp_loop_client").setLevel(getattr(logging, config["logging"]["level"]))

# DB ê²½ë¡œ í•´ê²° (ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜)
DB_RELATIVE_PATH = config.get("database", {}).get("path", "../db/agent_native_loop_data.db")
DB_PATH = (Path(__file__).parent / "agent_native_loop_config" / DB_RELATIVE_PATH).resolve()

# MCP í´ë¼ì´ì–¸íŠ¸ ì œê±° (ë¡œì»¬ ë„êµ¬ ì‚¬ìš©)
# mcp_client = McpSseClient(config["mcp"]["host"], db_path=DB_PATH)

def save_agent_log(request_id: str, message: str, details: Optional[str] = None):
    """DBì— ì—ì´ì „íŠ¸ í™œë™ ë¡œê·¸ ì €ì¥"""
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute(
            "INSERT INTO agent_logs (request_id, message, details) VALUES (?, ?, ?)",
            (request_id, message, details)
        )
        conn.commit()
        conn.close()
    except Exception as e:
        logger.error(f"âš ï¸ DB ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    logger.info("ğŸ¤– Agent Native Server ì‹œì‘ ì¤‘ (Truly Native Mode)...")
    logger.info(f"âœ… {len(NATIVE_TOOL_DEFS)}ê°œì˜ ë„¤ì´í‹°ë¸Œ ë„êµ¬ ë¡œë“œ ì™„ë£Œ")
    yield
    logger.info("ğŸ‘‹ Agent Native Server ì¢…ë£Œ")

app = FastAPI(title="Void Lab Test - Active Agent Native", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"])

# ìš”ì²­ ëª¨ë¸
class ChatMessage(BaseModel):
    role: str
    content: Optional[str] = None
    tool_calls: Optional[List[Dict[str, Any]]] = None
    tool_call_id: Optional[str] = None
    name: Optional[str] = None

class ChatRequest(BaseModel):
    model: Optional[str] = None
    messages: List[ChatMessage]
    tools: Optional[List[Dict[str, Any]]] = None
    stream: bool = False

@app.get("/v1/models")
async def list_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜ (Void IDE ì´ˆê¸°í™” ëŒ€ì‘)"""
    return {
        "object": "list",
        "data": [
            {
                "id": config["llm"]["model"],
                "object": "model",
                "created": int(datetime.now().timestamp()),
                "owned_by": config["llm"]["provider"]
            }
        ]
    }

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest):
    """
    ììœ¨ ì‹¤í–‰ ë£¨í”„ë¥¼ í¬í•¨í•œ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸
    """
    request_id = datetime.now().strftime("%H%M%S")
    logger.info(f"ğŸ“¥ [Agent-{request_id}] ìƒˆ ìš”ì²­ ìˆ˜ì‹ : {request.messages[-1].content}")
    save_agent_log(request_id, "Request Received", request.messages[-1].content)
    
    try:
        current_messages = [msg.model_dump(exclude_none=True) for msg in request.messages]
        
        # ë„êµ¬ ëª©ë¡ ë¡œë“œ
        tools = request.tools if request.tools else NATIVE_TOOL_DEFS
        
        # [HITL Feedback Loop Injection]
        # ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ ë„êµ¬ ì‹¤í–‰ ê²°ê³¼(role: tool)ì´ê³  ì‹¤íŒ¨(success: False)ì¸ ê²½ìš° 
        # LLMì—ê²Œ ìê°€ ìˆ˜ì •ì„ ìœ ë„í•˜ëŠ” ê°€ì´ë“œë¥¼ ì£¼ì…í•©ë‹ˆë‹¤.
        last_msg = current_messages[-1] if current_messages else None
        if last_msg and last_msg.get("role") == "tool":
            try:
                content_obj = json.loads(last_msg.get("content", "{}"))
                if isinstance(content_obj, dict) and not content_obj.get("success", True):
                    error_msg = content_obj.get("error", "Unknown error")
                    logger.warning(f"âš ï¸ [Agent-{request_id}] ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨ ê°ì§€ (HITL í”¼ë“œë°± ì£¼ì… ì¤‘)")
                    
                    # í”¼ë“œë°± ê°€ì´ë“œ ë©”ì‹œì§€ ìƒì„± (Ollama/vLLMì´ ì´ì „ ë„êµ¬ ê²°ê³¼ì˜ ì—°ì¥ì„ ìœ¼ë¡œ ì´í•´í•˜ë„ë¡ êµ¬ì„±)
                    feedback_guidance = f"\n\n[SYSTEM FEEDBACK]\në„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}\nì›ì¸ì„ ë¶„ì„í•˜ê³  í•„ìš”í•œ ê²½ìš° ìˆ˜ì •ëœ ì¸ìë¡œ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ë‹¤ë¥¸ ë°©ë²•ì„ ì°¾ì•„ì£¼ì„¸ìš”."
                    last_msg["content"] = last_msg.get("content", "") + feedback_guidance
                    save_agent_log(request_id, "Feedback Injected", error_msg)
            except Exception as e:
                logger.debug(f"ğŸ” [Agent-{request_id}] í”¼ë“œë°± ì£¼ì… ì‹œë„ ì‹¤íŒ¨: {e}")

        # [Single Turn Request]
        # ë‚´ë¶€ ë£¨í”„ë¥¼ ì œê±°í•˜ê³  LLMì—ê²Œ í•œ ë²ˆì˜ ì¶”ë¡ (Thinking)ì„ ìš”ì²­í•©ë‹ˆë‹¤.
        # ë„êµ¬ í˜¸ì¶œ(Tool Calls)ì´ ë°œìƒí•˜ë©´ Void IDEê°€ ì´ë¥¼ ìº¡ì²˜í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ìŠ¹ì¸(Accept)ì„ ìš”ì²­í•˜ê²Œ ë©ë‹ˆë‹¤.
        logger.info(f"ğŸ“¤ [Agent-{request_id}] [LLM REQ] LLMì—ê²Œ ë‹µë³€ ìš”ì²­ ì¤‘...")
        full_ollama_resp = await call_llm(current_messages, tools)
        
        logger.info(f"ğŸ“¥ [Agent-{request_id}] [LLM RESP] ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
        
        # ê²°ê³¼ ë°˜í™˜ (ìŠ¤íŠ¸ë¦¬ë° ì—¬ë¶€ì— ë”°ë¼)
        if request.stream:
            return StreamingResponse(
                generate_pseudo_stream_hitl(full_ollama_resp),
                media_type="text/event-stream"
            )
        else:
            return full_ollama_resp
        
    except Exception as e:
        logger.error(f"âŒ [Agent-{request_id}] ì²˜ë¦¬ ì¤‘ ì¹˜ëª…ì  ì—ëŸ¬: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

async def call_llm(messages: List[Dict], tools: Optional[List] = None):
    """LLM(Ollama, vLLM, OpenAI ë“±)ì˜ OpenAI í˜¸í™˜ API í˜¸ì¶œ"""
    async with httpx.AsyncClient(timeout=config["llm"]["timeout"]) as client:
        # OpenAI í˜¸í™˜ ì—”ë“œí¬ì¸íŠ¸
        url = f"{config['llm']['base_url']}/chat/completions"
        headers = {}
        api_key = str(config["llm"].get("api_key", "")).strip()
        # api_keyê°€ ì¡´ì¬í•˜ê³ , "not-needed"ê°€ ì•„ë‹ˆë©°, ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ í—¤ë” ì¶”ê°€
        if api_key and api_key.lower() != "not-needed":
            headers["Authorization"] = f"Bearer {api_key}"

        payload = {
            "model": config["llm"]["model"],
            "messages": messages,
            "stream": False,
            "temperature": 0
        }
        if tools:
            payload["tools"] = tools
            
        logger.debug(f"ğŸ“¡ [LLM TX] Payload:\n{json.dumps(payload, ensure_ascii=False, indent=2)}")
        resp = await client.post(url, json=payload, headers=headers)
        resp.raise_for_status()
        
        # OpenAI ê·œê²© ì‘ë‹µì—ì„œ message ì¶”ì¶œí•˜ì—¬ Ollama í˜•ì‹ê³¼ ë¹„ìŠ·í•˜ê²Œ ë°˜í™˜
        result = resp.json()
        return result

def generate_pseudo_stream_hitl(full_resp: Dict):
    """
    LLM ì‘ë‹µì„ OpenAI í˜¸í™˜ SSE ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì „ì†¡í•©ë‹ˆë‹¤.
    HITL ëª¨ë“œì—ì„œëŠ” tool_callsê°€ í¬í•¨ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì´ë¥¼ ê³ ë ¤í•©ë‹ˆë‹¤.
    """
    choice = full_resp.get("choices", [{}])[0]
    msg = choice.get("message", {})
    content = msg.get("content", "")
    tool_calls = msg.get("tool_calls", [])
    
    resp_id = full_resp.get("id", "hitl-" + datetime.now().strftime("%Y%m%d%H%M%S"))
    model_name = full_resp.get("model", config["llm"]["model"])
    created_time = full_resp.get("created", int(datetime.now().timestamp()))

    # 1. Start chunk (role)
    chunk = {
        "id": resp_id, "object": "chat.completion.chunk", "created": created_time, "model": model_name,
        "choices": [{"index": 0, "delta": {"role": "assistant"}, "finish_reason": None}]
    }
    yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"

    # 2. Content chunk (if any)
    if content:
        chunk = {
            "id": resp_id, "object": "chat.completion.chunk", "created": created_time, "model": model_name,
            "choices": [{"index": 0, "delta": {"content": content}, "finish_reason": None}]
        }
        yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"

    # 3. Tool Calls chunk (if any)
    if tool_calls:
        chunk = {
            "id": resp_id, "object": "chat.completion.chunk", "created": created_time, "model": model_name,
            "choices": [{"index": 0, "delta": {"tool_calls": tool_calls}, "finish_reason": None}]
        }
        yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"

    # 4. End chunk
    chunk = {
        "id": resp_id, "object": "chat.completion.chunk", "created": created_time, "model": model_name,
        "choices": [{"index": 0, "delta": {}, "finish_reason": choice.get("finish_reason", "stop")}]
    }
    yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
    yield "data: [DONE]\n\n"

def format_to_openai_response(ollama_resp: Dict):
    """Ollama ì‘ë‹µ í˜•ì‹ì„ OpenAI ê·œê²©ìœ¼ë¡œ ë³€í™˜"""
    choice = ollama_resp.get("choices", [{}])[0]
    message = choice.get("message", {})
    return {
        "id": "agent-" + datetime.now().strftime("%Y%m%d%H%M%S"),
        "object": "chat.completion",
        "created": int(datetime.now().timestamp()),
        "model": ollama_resp.get("model", config["llm"]["model"]),
        "choices": [
            {
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": message.get("content", "")
                },
                "finish_reason": "stop"
            }
        ],
        "usage": {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0
        }
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host=config["agent"]["host"], port=config["agent"]["port"])
