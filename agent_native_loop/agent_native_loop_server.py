"""
# agent_native_loop_server.py - ììœ¨ì ìœ¼ë¡œ ë„êµ¬ë¥¼ ì‹¤í–‰í•˜ëŠ” ëŠ¥ë™ì  ëŒ€ë¦¬ì¸ ì„œë²„ (Native ë²„ì „)

LLMì´ ë„êµ¬ í˜¸ì¶œì„ ìš”ì²­í•˜ë©´, í´ë¼ì´ì–¸íŠ¸(Void)ì—ê²Œ ë°˜í™˜í•˜ê¸° ì „ì— 
ì§ì ‘ MCP ì„œë²„ì™€ í†µì‹ í•˜ì—¬ ë„êµ¬ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ LLMì—ê²Œ ë‹¤ì‹œ ì „ë‹¬í•©ë‹ˆë‹¤.
ìµœì¢… ë‹µë³€ì´ ë‚˜ì˜¬ ë•Œê¹Œì§€ ì´ ê³¼ì •ì„ ë°˜ë³µí•©ë‹ˆë‹¤.
"""

import json
import logging
import sys
import sqlite3
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional

import httpx
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
from pydantic import BaseModel

# ë¡œì»¬ ëª¨ë“ˆ
# ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ë¥¼ ê²½ë¡œì— ì¶”ê°€í•˜ì—¬ ì–´ë””ì„œ ì‹¤í–‰í•˜ë“  native_toolsë¥¼ ì°¾ì„ ìˆ˜ ìˆê²Œ í•¨
sys.path.append(str(Path(__file__).parent))
from native_loop_tools import NATIVE_TOOL_DEFS, NATIVE_TOOL_REGISTRY

# ì„¤ì • ë¡œë“œ
CONFIG_PATH = (Path(__file__).parent / "agent_native_loop_config" / "agent_native_loop_config.json").resolve()

def load_config():
    with open(CONFIG_PATH, encoding="utf-8") as f:
        config = json.load(f)
        
    # í”„ë¡œíŒŒì¼ ì§€ì›: active_profileì´ ìˆìœ¼ë©´ í•´ë‹¹ ì„¤ì •ì„ llm ì„¹ì…˜ìœ¼ë¡œ ë³µì‚¬
    if "active_profile" in config and "llm_profiles" in config:
        active = config["active_profile"]
        if active in config["llm_profiles"]:
            config["llm"] = config["llm_profiles"][active]
            
    return config

config = load_config()

# ë¡œê¹… ì„¤ì •
LOG_FILE = (Path(__file__).parent / "agent_native_loop.log").resolve()
logging.basicConfig(
    level=getattr(logging, config["logging"]["level"]),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(LOG_FILE, encoding="utf-8")
    ]
)
logger = logging.getLogger("agent_native")
# mcp_client ë¡œê±°ë„ ê°™ì€ í•¸ë“¤ëŸ¬ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì • (ìƒì†)
logging.getLogger("mcp_client").setLevel(getattr(logging, config["logging"]["level"]))

# DB ê²½ë¡œ í•´ê²° (ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜)
DB_RELATIVE_PATH = config.get("database", {}).get("path", "../db/agent_native_loop_data.db")
DB_PATH = (Path(__file__).parent / "agent_native_loop_config" / DB_RELATIVE_PATH).resolve()

# MCP í´ë¼ì´ì–¸íŠ¸ ì œê±° (ë¡œì»¬ ë„êµ¬ ì‚¬ìš©)
# mcp_client = McpSseClient(config["mcp"]["host"], db_path=DB_PATH)

def save_agent_log(request_id: str, message: str, details: Optional[str] = None):
    """DBì— ì—ì´ì „íŠ¸ í™œë™ ë¡œê·¸ ì €ì¥"""
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute(
            "INSERT INTO agent_logs (request_id, message, details) VALUES (?, ?, ?)",
            (request_id, message, details)
        )
        conn.commit()
        conn.close()
    except Exception as e:
        logger.error(f"âš ï¸ DB ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    logger.info("ğŸ¤– Agent Native Server ì‹œì‘ ì¤‘ (Truly Native Mode)...")
    logger.info(f"âœ… {len(NATIVE_TOOL_DEFS)}ê°œì˜ ë„¤ì´í‹°ë¸Œ ë„êµ¬ ë¡œë“œ ì™„ë£Œ")
    yield
    logger.info("ğŸ‘‹ Agent Native Server ì¢…ë£Œ")

app = FastAPI(title="Void Lab Test - Active Agent Native", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"])

# ìš”ì²­ ëª¨ë¸
class ChatMessage(BaseModel):
    role: str
    content: Optional[str] = None
    tool_calls: Optional[List[Dict[str, Any]]] = None
    tool_call_id: Optional[str] = None
    name: Optional[str] = None

class ChatRequest(BaseModel):
    model: Optional[str] = None
    messages: List[ChatMessage]
    tools: Optional[List[Dict[str, Any]]] = None
    stream: bool = False

@app.get("/v1/models")
async def list_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜ (Void IDE ì´ˆê¸°í™” ëŒ€ì‘)"""
    return {
        "object": "list",
        "data": [
            {
                "id": config["llm"]["model"],
                "object": "model",
                "created": int(datetime.now().timestamp()),
                "owned_by": config["llm"]["provider"]
            }
        ]
    }

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest):
    """
    ììœ¨ ì‹¤í–‰ ë£¨í”„ë¥¼ í¬í•¨í•œ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸
    """
    request_id = datetime.now().strftime("%H%M%S")
    logger.info(f"ğŸ“¥ [Agent-{request_id}] ìƒˆ ìš”ì²­ ìˆ˜ì‹ : {request.messages[-1].content}")
    save_agent_log(request_id, "Request Received", request.messages[-1].content)
    
    try:
        current_messages = [msg.model_dump(exclude_none=True) for msg in request.messages]
        
        # ë„êµ¬ ëª©ë¡ ë¡œë“œ (ë¡œì»¬ native_tools ì‚¬ìš©)
        tools = request.tools
        if not tools:
            logger.info(f"ğŸ” [Agent-{request_id}] ë¡œì»¬ ë„¤ì´í‹°ë¸Œ ë„êµ¬ ëª©ë¡ ì‚¬ìš© ì¤‘...")
            tools = NATIVE_TOOL_DEFS
            logger.info(f"ğŸ“¦ [Agent-{request_id}] {len(tools)}ê°œì˜ ë„¤ì´í‹°ë¸Œ ë„êµ¬ ë°œê²¬")
        
        # --------------------------------------------------------
        # ğŸ”„ Autonomous Agent Loop (n8n ìŠ¤íƒ€ì¼ì˜ ìƒíƒœ ë¨¸ì‹ )
        # --------------------------------------------------------
        # ì´ ë£¨í”„ëŠ” n8n AI Agent ë…¸ë“œì˜ 'Looping & State Machine' ì•„í‚¤í…ì²˜ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
        # ë‹¨ìˆœíˆ ê²°ê³¼ë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ìŠ¤ìŠ¤ë¡œ ë‹¤ìŒ í–‰ë™ì„ ê²°ì •í•˜ê³  ì‹¤í–‰í•˜ëŠ” ëŠ¥ë™ì  êµ¬ì¡°ì…ë‹ˆë‹¤.
        max_iterations = 5
        for i in range(max_iterations):
            logger.info(f"ğŸ”„ [Agent-{request_id}] ë°˜ë³µ {i+1}ë‹¨ê³„ ì‹¤í–‰ ì¤‘...")
            
            # [ìƒíƒœ 1: Thinking] LLMì—ê²Œ í˜„ì¬ê¹Œì§€ì˜ ëŒ€í™” ì´ë ¥ì„ ì „ë‹¬í•˜ì—¬ 'ìƒê°'ì„ ìš”ì²­í•©ë‹ˆë‹¤.
            # n8nì˜ "AI Agent Node"ê°€ LLM ëª¨ë¸ì— ì§ˆë¬¸ì„ ë˜ì§€ëŠ” ê³¼ì •ê³¼ ë™ì¼í•©ë‹ˆë‹¤.
            logger.info(f"ğŸ“¤ [Agent-{request_id}] [LLM REQ] LLMì—ê²Œ ë‹µë³€ ìš”ì²­ ì¤‘...")
            full_ollama_resp = await call_llm(current_messages, tools)
            
            logger.info(f"ğŸ“¥ [Agent-{request_id}] [LLM RESP] ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
            logger.debug(f"--- [LLM RESP Detail] ---\n{json.dumps(full_ollama_resp, ensure_ascii=False, indent=2)}\n-------------------------")

            choice = full_ollama_resp.get("choices", [{}])[0]
            message = choice.get("message", {})
            tool_calls = message.get("tool_calls", [])
            content = message.get("content", "")
            
            # [ìƒíƒœ 2: Fallback/Analysis] ëª¨ë¸ì˜ ì‘ë‹µì´ ê·œê²©í™”ëœ tool_callsì¸ì§€, í˜¹ì€ í…ìŠ¤íŠ¸ ë‚´ JSONì¸ì§€ ë¶„ì„í•©ë‹ˆë‹¤.
            # n8nì´ LLM ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ ë‹¤ìŒ ë…¸ë“œ(ë„êµ¬)ë¥¼ ì‹¤í–‰í• ì§€ ê²°ì •í•˜ëŠ” "Output Parser" ë‹¨ê³„ì…ë‹ˆë‹¤.
            if not tool_calls and content:
                # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±° ë° JSON ì¶”ì¶œ ì‹œë„
                json_str = content.strip()
                if "```json" in json_str:
                    match = re.search(r"```json\s*(\{.*?\})\s*```", json_str, re.DOTALL)
                    json_str = match.group(1) if match else json_str
                elif "```" in json_str:
                    match = re.search(r"```\s*(\{.*?\})\s*```", json_str, re.DOTALL)
                    json_str = match.group(1) if match else json_str
                
                # ì¤‘ê´„í˜¸ ë²”ìœ„ë¥¼ ì°¾ì•„ JSONë§Œ ì¶”ì¶œ (ê°€ì¥ ë°”ê¹¥ìª½ { })
                if "{" in json_str and "}" in json_str:
                    start_idx = json_str.find("{")
                    # ë‹¨ìˆœ find/rfindëŠ” ì¤‘ì²©ëœ ì¤‘ê´„í˜¸ì—ì„œ ìœ„í—˜í•  ìˆ˜ ìˆì§€ë§Œ, 
                    # ì—¬ê¸°ì„œëŠ” ê°€ì¥ ë°”ê¹¥ìª½ íŒ¨í„´ì„ ì°¾ê¸° ìœ„í•´ ì‹œë„
                    # ë” ì •êµí•˜ê²ŒëŠ” ê´„í˜¸ ë§¤ì¹­ì„ í•´ì•¼ í•¨
                    temp_str = json_str[start_idx:]
                    depth = 0
                    end_idx = -1
                    for idx, char in enumerate(temp_str):
                        if char == '{': depth += 1
                        elif char == '}':
                            depth -= 1
                            if depth == 0:
                                end_idx = idx
                                break
                    if end_idx != -1:
                        json_str = temp_str[:end_idx+1]

                try:
                    potential_tool = json.loads(json_str)
                    if "name" in potential_tool and "arguments" in potential_tool:
                        tool_calls = [{
                            "id": f"call_{i}_{datetime.now().strftime('%M%S')}",
                            "type": "function",
                            "function": {
                                "name": potential_tool["name"],
                                "arguments": json.dumps(potential_tool["arguments"]) if isinstance(potential_tool["arguments"], dict) else potential_tool["arguments"]
                            }
                        }]
                        message["tool_calls"] = tool_calls
                        logger.info(f"ğŸ’¡ [Agent-{request_id}] Contentì—ì„œ JSON ë„êµ¬ í˜¸ì¶œ ì¶”ì¶œ ì™„ë£Œ!")
                except Exception as e:
                    logger.debug(f"ğŸ” [Agent-{request_id}] JSON ì¶”ì¶œ ì‹œë„ ì‹¤íŒ¨: {e}")

            # ë„êµ¬ í˜¸ì¶œì´ ìˆìœ¼ë©´ contentë¥¼ ë¹„ì›Œì¤Œ (ëª¨ë¸ì— ë”°ë¼ ì¤‘ë³µìœ¼ë¡œ ì¸ì‹í•  ìˆ˜ ìˆìŒ)
            if tool_calls:
                message["content"] = ""

            # [ìƒíƒœ 3: Exit Condition] ë„êµ¬ í˜¸ì¶œì´ ì—†ìœ¼ë©´ ì—ì´ì „íŠ¸ê°€ "í•  ì¼ì„ ë‹¤ í–ˆë‹¤"ê³  íŒë‹¨í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒíƒœê°€ ë©ë‹ˆë‹¤.
            # n8n ì›Œí¬í”Œë¡œìš°ê°€ ìµœì¢… 'Response' ì¶œë ¥ì„ ë‚´ë³´ë‚´ëŠ” ì§€ì ì…ë‹ˆë‹¤.
            if not tool_calls:
                logger.info(f"âœ… [Agent-{request_id}] ìµœì¢… ì‘ë‹µ ë„ë‹¬")
                final_resp = format_to_openai_response(full_ollama_resp)
                
                if request.stream:
                    logger.info(f"ğŸ“¡ [Agent-{request_id}] ìŠ¤íŠ¸ë¦¬ë° í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜")
                    return StreamingResponse(
                        generate_pseudo_stream(final_resp),
                        media_type="text/event-stream"
                    )
                else:
                    return final_resp
            
            # [ìƒíƒœ 4: Action/Execution] ëª¨ë¸ì´ ìš”ì²­í•œ ë„êµ¬ë“¤ì„ ì‹¤ì œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.
            # ì´ ë¶€ë¶„ì´ Void IDEì™€ ê°€ì¥ í° ì°¨ë³„ì ìœ¼ë¡œ, ì‚¬ìš©ìì˜ í´ë¦­ ì—†ì´ ì„œë²„ê°€ 'ìë™ ì‹¤í–‰'ì„ ìˆ˜í–‰í•˜ëŠ” n8nì˜ Executor ì—­í• ì…ë‹ˆë‹¤.
            logger.info(f"ğŸ”§ [Agent-{request_id}] LLMì´ {len(tool_calls)}ê°œì˜ ë„êµ¬ í˜¸ì¶œ ìš”ì²­")
            current_messages.append(message) # LLMì˜ ë„êµ¬ ìš”ì²­ ë©”ì‹œì§€ ì¶”ê°€ (History Update)
            
            for tool_call in tool_calls:
                func_name = tool_call["function"]["name"]
                args = json.loads(tool_call["function"]["arguments"])
                call_id = tool_call.get("id")
                
                logger.info(f"ğŸ› ï¸  [Agent-{request_id}] [NATIVE TOOL CALL] {func_name} ì‹œì‘")
                logger.info(f"   â†’ ì¸ì(Args): {args} [ID: {call_id}]")
                save_agent_log(request_id, f"Native Tool Call: {func_name}", json.dumps(args))
                
                # ë¡œì»¬ ë„¤ì´í‹°ë¸Œ ë„êµ¬ ì§ì ‘ ì‹¤í–‰ (MCP ì„œë²„ í˜¸ì¶œ ì—†ìŒ)
                if func_name in NATIVE_TOOL_REGISTRY:
                    try:
                        # ë™ê¸° í•¨ìˆ˜ì¸ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ì²˜ë¦¬ (í˜„ì¬ëŠ” ëª¨ë‘ ë™ê¸°)
                        result = NATIVE_TOOL_REGISTRY[func_name](**args)
                    except Exception as e:
                        result = {"success": False, "error": str(e)}
                else:
                    result = {"success": False, "error": f"ì •ì˜ë˜ì§€ ì•Šì€ ë„êµ¬: {func_name}"}
                
                logger.info(f"âœ… [Agent-{request_id}] [NATIVE TOOL RESULT] {func_name} ì™„ë£Œ")
                logger.debug(f"   â†’ ê²°ê³¼: {json.dumps(result, ensure_ascii=False)}")
                
                # [ìƒíƒœ 5: Feedback/State Update] ë„êµ¬ ì‹¤í–‰ ê²°ê³¼(Observation)ë¥¼ ëŒ€í™” ì´ë ¥ì— ì¶”ê°€í•©ë‹ˆë‹¤.
                # role: "tool"ì„ í†µí•´ ëª¨ë¸ì—ê²Œ "ì´ê²ƒì€ ë„¤ê°€ ì‹œí‚¨ í–‰ë™ì˜ ê²°ê³¼ì•¼"ë¼ê³  ì•Œë ¤ì¤ë‹ˆë‹¤.
                # ì´ë¥¼ í†µí•´ ë‹¤ìŒ ë£¨í”„(ìƒíƒœ 1)ì—ì„œ ëª¨ë¸ì€ ì´ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ í–‰ë™ì„ ê²°ì •í•˜ê²Œ ë©ë‹ˆë‹¤.
                
                # Feedback Loop: ê²°ê³¼ê°€ ì‹¤íŒ¨ì¸ ê²½ìš°, ëª¨ë¸ì—ê²Œ ëª…ì‹œì ìœ¼ë¡œ ìˆ˜ì •ì„ ìš”ì²­í•˜ëŠ” í”„ë¡¬í”„íŠ¸ ì¶”ê°€ ê°€ëŠ¥
                if not result.get("success", True):
                    error_msg = result.get("error", "Unknown error")
                    logger.warning(f"âš ï¸  [Agent-{request_id}] ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨ ê°ì§€: {func_name}")
                    
                    feedback_content = f"ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}\nì›ì¸ì„ ë¶„ì„í•˜ê³  í•„ìš”í•œ ê²½ìš° ìˆ˜ì •ëœ ì¸ìë¡œ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ë‹¤ë¥¸ ë°©ë²•ì„ ì°¾ì•„ì£¼ì„¸ìš”."
                    current_messages.append({
                        "role": "tool",
                        "tool_call_id": call_id,
                        "content": json.dumps({"status": "error", "message": feedback_content, "raw_result": result}, ensure_ascii=False)
                    })
                else:
                    current_messages.append({
                        "role": "tool",
                        "tool_call_id": call_id,
                        "content": json.dumps(result, ensure_ascii=False)
                    })
                
            # [Loop Back] ë£¨í”„ì˜ ì²˜ìŒ(ìƒíƒœ 1)ìœ¼ë¡œ ëŒì•„ê°€ ì •ë³´ë¥¼ ì£¼ì…ë°›ì€ LLMì˜ ë‹¤ìŒ íŒë‹¨ì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
        
        raise HTTPException(status_code=500, detail="ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ì´ˆê³¼")
        
    except Exception as e:
        logger.error(f"âŒ [Agent-{request_id}] ì²˜ë¦¬ ì¤‘ ì¹˜ëª…ì  ì—ëŸ¬: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

async def call_llm(messages: List[Dict], tools: Optional[List] = None):
    """LLM(Ollama, vLLM, OpenAI ë“±)ì˜ OpenAI í˜¸í™˜ API í˜¸ì¶œ"""
    async with httpx.AsyncClient(timeout=config["llm"]["timeout"]) as client:
        # OpenAI í˜¸í™˜ ì—”ë“œí¬ì¸íŠ¸
        url = f"{config['llm']['base_url']}/chat/completions"
        headers = {}
        headers = {}
        api_key = str(config["llm"].get("api_key", "")).strip()
        # api_keyê°€ ì¡´ì¬í•˜ê³ , "not-needed"ê°€ ì•„ë‹ˆë©°, ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ í—¤ë” ì¶”ê°€
        if api_key and api_key.lower() != "not-needed":
            headers["Authorization"] = f"Bearer {api_key}"

        payload = {
            "model": config["llm"]["model"],
            "messages": messages,
            "stream": False,
            "temperature": 0
        }
        if tools:
            payload["tools"] = tools
            
        logger.debug(f"ğŸ“¡ [LLM TX] Payload:\n{json.dumps(payload, ensure_ascii=False, indent=2)}")
        resp = await client.post(url, json=payload, headers=headers)
        resp.raise_for_status()
        
        # OpenAI ê·œê²© ì‘ë‹µì—ì„œ message ì¶”ì¶œí•˜ì—¬ Ollama í˜•ì‹ê³¼ ë¹„ìŠ·í•˜ê²Œ ë°˜í™˜
        result = resp.json()
        return result

def generate_pseudo_stream(final_resp: Dict):
    """ì¼ë°˜ ì‘ë‹µì„ SSE ìŠ¤íŠ¸ë¦¼ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    # ì²« ë²ˆì§¸ ì²­í¬: roleë§Œ ì „ì†¡
    chunk1 = {
        "id": final_resp["id"],
        "object": "chat.completion.chunk",
        "created": final_resp["created"],
        "model": final_resp["model"],
        "choices": [
            {
                "index": 0,
                "delta": {"role": "assistant"},
                "finish_reason": None
            }
        ]
    }
    yield f"data: {json.dumps(chunk1, ensure_ascii=False)}\n\n"
    
    # ë‘ ë²ˆì§¸ ì²­í¬: content ì „ì†¡
    chunk2 = {
        "id": final_resp["id"],
        "object": "chat.completion.chunk",
        "created": final_resp["created"],
        "model": final_resp["model"],
        "choices": [
            {
                "index": 0,
                "delta": {"content": final_resp["choices"][0]["message"]["content"]},
                "finish_reason": None
            }
        ]
    }
    yield f"data: {json.dumps(chunk2, ensure_ascii=False)}\n\n"
    
    # ì„¸ ë²ˆì§¸ ì²­í¬: finish_reason
    chunk3 = {
        "id": final_resp["id"],
        "object": "chat.completion.chunk",
        "created": final_resp["created"],
        "model": final_resp["model"],
        "choices": [
            {
                "index": 0,
                "delta": {},
                "finish_reason": "stop"
            }
        ]
    }
    yield f"data: {json.dumps(chunk3, ensure_ascii=False)}\n\n"
    yield "data: [DONE]\n\n"

def format_to_openai_response(ollama_resp: Dict):
    """Ollama ì‘ë‹µ í˜•ì‹ì„ OpenAI ê·œê²©ìœ¼ë¡œ ë³€í™˜"""
    choice = ollama_resp.get("choices", [{}])[0]
    message = choice.get("message", {})
    return {
        "id": "agent-" + datetime.now().strftime("%Y%m%d%H%M%S"),
        "object": "chat.completion",
        "created": int(datetime.now().timestamp()),
        "model": ollama_resp.get("model", config["llm"]["model"]),
        "choices": [
            {
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": message.get("content", "")
                },
                "finish_reason": "stop"
            }
        ],
        "usage": {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0
        }
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host=config["agent"]["host"], port=config["agent"]["port"])
